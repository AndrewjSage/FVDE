---
title: "FVDE Progress Report 2"
author: vub23 & mina-cheese
format: pdf
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = F, message=FALSE, warning=FALSE, fig.height = 10)
options(scipen = 5)
library(tidyverse)
library(stargazer)
library(corrplot)
library(patchwork)
library(tigris)
library(sf)
library(factoextra)
library(kableExtra)
```

This project will contain the progress we have made over the last week and a half. Per Prof. Sage's advice, we have chosen to go with a new scoring paradigm that ranks census tracts from 0 to 100 in comparison to national standards, instead of relying on z-scores. We will also look at the outliers of each vital index.

\newpage

# Humane Housing

For this report, I've chosen 5 variables that are the closest proxies to more humane housing outcomes and created my own scoring system that ranges from 0 to 100. Most of my rationale are included below. Additionally, differences in variable distributions and scores moving from Zipcodes to Census Tracts divisions will be mentioned.

## Census Tracts Dataset:

```{r}
# Read csv file
hh <- read.csv("hh tracts 2.csv")
hh <- hh %>% select(-Layer, -Name)

# Copying Mina's code for the string pruning process
names(hh) <- sub("_.*$", "", names(hh))

# Creating other dataframes that fit the requirements
hhCorr <- hh %>% select(-HCP) %>% na.omit()
hhSumm <- hh %>% select(-GEOID)
```

### Correlation plots and Histograms:

Below is the correlation plot for numeric variables:

```{r corrplot, fig.width=10, fig.height=10}
A = cor(hhCorr)
corrplot(A, method='color', order = 'AOE', type='upper', addCoef.col = 'black')
```

We can notice the decrease in the number of strong correlations in comparison to the zipcodes dataset. Most variable-pairs now exhibit only some to no correlation at all. Notable correlations that weren't noted in the first report are noted below:

-   `EJV`, the vulnerable demographics index, is negatively correlated with home ownership rates `HUO` and median income `INC`, which is understandable: with higher income, people can afford to buy instead of rent, raising `HUO`, and will not be as vulnerable, resulting in a lower demographic vulnerability index.

-   `FAI`, the metric for food insecurity, shows the same negative correlations with income and home ownership, which is similarly explained.

-   Median household income is also positively correlated with housing cost variables `MHC`, `RNT`, which is expected: the more you earn, the better your housing situation will likely be.

-   Food insecurity and demographic vulnerability also show a strong positive correlation.

```{r, fig.width=10}
bvars <- names(hhSumm)
hhbins <- function(x) {
  diff(range(x, na.rm = TRUE)) / (2 * IQR(x, na.rm = TRUE) / length(x)^(1/3))
} #using the Freedman-Diaconis rule to compute the binwidth

hhplots <- map(bvars, ~{
  varname <- .x
  binwidth <- round(hhbins(hhSumm[[varname]]))
  
  ggplot(data=hhSumm, aes_string(x=varname)) + 
    geom_histogram(fill="#2196f3", color="#000000", bins=binwidth) + 
    labs(title=varname, x=NULL, y="Count")
})

wrap_plots(hhplots, ncol = 4, nrow = 5) & plot_annotation(caption="*Note: histogram binwidth was decided using the Freedman-Diaconis rule.")
```

The histograms show that even with changing the geographic divisions, most variables show the same trends and distributions. Most variables have a right-tailed distribution, save for `HUO` and `APR`, which are housing ownership rates and mortgage approval rates.

### Summary Statistics:

The table for summary statistics are shown below:

```{r, results='asis'}
stargazer(hhSumm, title="Summary Stats", type = 'latex', summary.stat = c("n","mean","sd", "min", "p25", "median", "p75", "max"), font.size = "small", notes.append=F, header=F)
```

## New Index Creation:

### Variable Selection Process:

Per our discussions last time, our index would contain variables that encompasses affordability, safety, stability, diversity and accessibility. In the time constraint given, I have selected 5 variables that cover most of our bases, which are:

### Variable 1: Housing Costs:

Absolute rent levels mean nothing if we are talking about a national scale - the sole addition of Manhattan's tracts will throw our entire proposed scale off balance. If this is the case, we should probably think of something a little more income-based. Using 2 variables: median housing costs `MHC` and median household income `INC` (from a new hh census tracts file), we can find the housing cost to income ratio.

Some searches reveal that US' best cities tend to have a rent-to-income ratio of \~13% while the worst ones (New York, Miami, Seattle) would be way north of 50%, even 60%. Knowing that MHC is inclusive of mortgages, but also other fees and taxes, I would push this estimate a little to the right: our "ideal" would be any tract with the ratio less than 15%, our "terrible" would be any tract with a ratio of 50% or higher. 30% is proposed as the highest proportion of monthly income the average person should spend on rent, so we would leave it as our "average". That leaves us with 23% as the "good" benchmark and 40% as the "bad" threshold.

With the new paradigm in place, here is a table showing the distribution of the score:

```{r, results='asis'}
# Creating another column calculating the ratio:
hh <- hh %>% mutate(icPct = MHC/(INC/12)*100, icScore = approx(x=c(15,23,30,40,50), y=c(100,75,50,25,0), xout=icPct, rule=2)$y)

score1 <- hh %>% select(icScore)
stargazer(score1, type = 'latex', summary.stat = c("n","mean","sd", "min", "median", "max"), notes.append=F, header=F)
```

A median value of almost 96 â€“\> Very right skewed. However, this tracks with our expectations of Wisconsin/the Midwest in general. We do have very affordable housing in comparison with other areas in the US.

### Variable 2: Eviction Rates

This is also a good proxy for housing availability and stability. However, there are some nuances and considerations to account for working with eviction rates:

-   Eviction can mean many things: it can be the filing of the legal process that removes tenants from a property, it can also be the actual removal of such residents from their homes.

-   Some jurisdictions allow landlords to file multiple eviction notices, strongly skewing extreme values (some Maryland counties report a 147% eviction rate, for example).

-   However, the bulk of values (both external and from the dataset) seem to be on the low end, so the thresholds will reflect that.

Currently, the thresholds for ideal/good/average/bad/terrible are: 0%/1%/3%/7%/13%. Similarly, here is a table showing the summary statistics of this new metric:

```{r, results='asis'}
hh <- hh %>% mutate(evScore = approx(x=c(0,1,3,7,13), y=c(100,75,50,25,0), xout=EVR, rule=2)$y)
score2 <- hh %>% select(evScore)
stargazer(score2, type = 'latex', summary.stat = c("n","mean","sd", "min", "median", "max"), notes.append=F, header=F)
```

### Variable 3: Home Ownership Rates

As noted by Jason, home ownership rates is a very important metric for humane housing, partly because it's a nice proxy to more humane conditions. There are still some considerations regarding this variable though:

-   Higher `HUO` rates might indicate higher neighborhood stability, but also lack of rental housing.

-   Tracts and areas with transient populations (like dorms and institutional quarters) might have disproportionately low levels of `HUO`, and therefore should be addressed appropriately. I am currently choosing to skip these outliers.

Similar to other variables, I am including the summary statistics table below:

```{r, results='asis'}
hh <- hh %>% mutate(hoScore = case_when(
      HUO < 20 ~ NA_real_,
      TRUE ~ approx(
        x    = c(20, 40, 60, 80, 90),
        y    = c(0, 20, 50, 80, 100),
        xout = HUO,
        rule = 2
      )$y))
score3 <- hh %>% select(hoScore)
stargazer(score3, type = 'latex', summary.stat = c("n","mean","sd", "min", "median", "max"), notes.append=F, header=F)
```

### Variable 4: Mean Travel Time

This variable seems to be the most straightforward to create an index from. Some research has confirmed that any commute less than 16 minutes is ideal, and 60+ commutes to be harmful, so we know where to place our thresholds. Here is another table that shows the distribution of our score:

```{r, results='asis'}
hh <- hh %>% mutate(trScore = approx(
        x    = c(16, 20, 25, 35, 60),
        y    = c(100,  80, 50, 20,0),
        xout = TRV,
        rule = 2
      )$y)
score4 <- hh %>% select(trScore)
stargazer(score4, type = 'latex', summary.stat = c("n","mean","sd", "min", "median", "max"), notes.append=F, header=F)
```

### Variable 5: Diversity

The last variable explicitly mentioned by Jason is that of racial/ethnic diversity. Though arguments can be made against our current prior that more diversity is conducive to more humane housing, we are still pursuing such a paradigm. Knowing that the highest possible value is 0.875, we should turn to other sources of data and examine the ranges of possible values.

The biggest, most diverse cities in the country like New York and LA tend to have a `REX` of 0.75 and higher. Conversely, counties like Laredo, Texas, with a predominantly hispanic population would only come in at 0.1 or lower on the scale. Given that this is the case, my current suggestion for the thresholds are 0.15, 0.25, 0.5, 0.65 and 0.75 for conditions ranging from terrible to ideal. Below is a table showing us the distribution of `REX` scores across different tracts of our dataset.

```{r, results='asis'}
hh <- hh %>%
  mutate(diScore = approx(
    x    = c(0.15, 0.25, 0.50, 0.65, 0.75),
    y    = c(0,    20,   50,   80,  100),
    xout = REX,
    rule = 2
  )$y)
score5 <- hh %>% select(diScore)
stargazer(score5, type = 'latex', summary.stat = c("n","mean","sd", "min", "median", "max"), notes.append=F, header=F)
```

\newpage

## New Index Analysis:

With our 5 variables selected, here is a map containing the average of the new scores for each census tracts and a table containing the top and bottom 10 tracts along with individual scores:

```{r, fig.width=10, fig.height=10}
# Arbitrary limits decided by Mina (so, not arbitrary)
zoom_xlim <- c(-89, -88)
zoom_ylim <- c(43.8, 44.7)

ind <- hh %>%  select(GEOID, ends_with("Score")) %>%  
  rowwise() %>%  mutate(newIndex = mean(c_across(ends_with("Score")), na.rm = TRUE)) %>% 
  ungroup() %>%  arrange(desc(newIndex))

# Getting spatial data from tigris, zcta was found from reading the documentation
zips <- unique(hh$GEOID)
spatial <- tracts(state="wi", class="sf", year=2023, progress_bar=F)
foxZips <- spatial %>% mutate(GEOID=as.character(GEOID)) %>% filter(GEOID %in% zips)
# Getting other zips from tigris so that we know where we are
background <- spatial %>%
  mutate(GEOID=as.character(GEOID)) %>%
  filter(!GEOID %in% zips)

# joining the data tables together
ind <- ind %>% mutate(GEOID=as.character(GEOID)) %>%  left_join(foxZips, by=c("GEOID"="GEOID"))

ind <- ind %>% st_as_sf()


# Drawing up the map
ggplot() +
  geom_sf(data = ind, aes(fill = newIndex), color="black") +
  geom_sf(data = background, fill = "grey95", color = "grey80", linewidth = 0.1) +
  scale_fill_viridis_c() + 
  labs(title = "Map of Census Tracts by New Housing Index") +
  theme_minimal() + coord_sf(xlim = zoom_xlim, ylim = zoom_ylim, expand = FALSE)
```

\newpage

```{r}
# Drop geometry, rename columns, keep only the scores
scoreTbl <- ind %>% 
  st_drop_geometry() %>% 
  rename(
    Tract           = GEOID,
    CostIncome = icScore,
    Eviction   = evScore,
    Ownership  = hoScore,
    Commute   = trScore,
    Diversity  = diScore,
    Overall    = newIndex
  ) %>% 
  select(Tract, CostIncome, Eviction, Ownership,
         Commute, Diversity, Overall)

# Slice out top-10 and bottom-10
top10    <- scoreTbl %>% arrange(desc(Overall)) %>% slice_head(n = 10)
bottom10 <- scoreTbl %>% arrange(Overall)        %>% slice_head(n = 10)

# Render with kable, putting most args on one line
knitr::kable(top10,    caption = "Top 10 Tracts",    digits = 1, booktabs = TRUE) %>% 
  kable_styling(full_width = FALSE)


knitr::kable(bottom10, caption = "Bottom 10 Tracts", digits = 1, booktabs = TRUE) %>% 
  kable_styling(full_width = FALSE)
```

There are some observations about the best and worst humane housing performers we can make:

-   The census tract with the highest humane housing index is tract 9400, which is on the Oneida reservation. It led other census tracts not only in terms of affordability but also diversity, which is the reason why it got the highest average score.

-   Looking at other tracts in the top 10 table, we can see that they are performing well not because of, but in spite of the diversity score: 6/10 tracts have a diversity index of less than 30/100. This is expected for small areas in a state like Wisconsin, potentially meriting some further investigation into the best way to account for diversity.

-   As for census tracts in the bottom 10, a consistent trend among these areas is low home ownership scores. Though the metric can be distorted by the presence of institution quarters and dorms, they also imply housing instability and understandably less humane housing outcomes.

-   There are also tracts in the bottom 10 which are designated opportunity zones - less fortunate areas which have received federal assistance and funding.

\newpage

# Lifelong Learning

We look into lifelong learning for the census tract subdivision.

```{r}
#file orwhatever
# getting the data into readable format
colnames <- names(read.csv("Lifelong-Learning-Cencus-Tracts.csv",nrows=0))

# data wrangling blah blah blah
balls <- read.csv("Lifelong-Learning-Cencus-Tracts.csv", skip=2, header=FALSE)
names(balls) <- colnames
balls <- balls |> select(-Layer) # removing redundant columns
names(balls)[4:8] <- c("EDA", "EDB", "EDC", "EDE", "EDG") # get rid of the stupid date things since they're all the same (see below)

# data for the prelim analysis
ballsAnal <- balls |> select(-GEOID, -Name)
```

## Legend

Similar to the zip code level, the census tract variable names do not provide much insight into what they are representing. Thus, I will describe them here. Please note that all of this data was collected from 2019-2023.

-   `EDA`: 9th grade education rate (% of residents)
-   `EDB`: High school graduation rate (% of residents)
-   `EDC`: Any higher education rate (% of residents)
-   `EDE`: College graduation rate (% of residents)
-   `EDG`: Preschool enrollment rate (% of toddlers ages 3-4)

Good news! There are basically no `NA` entries in this dataset at all, so we do not need to worry about that!! The one `NA` entry is in the preschool enrollment rate `EDG` in census tract 7. Also worth noting that there are a lot of 0 entries in that column. Not sure why that is.

## Exploratory Plots and Variables

### Correlation Plots

```{r, fig.width=6, fig.height=6}
# there is one NA entry in EDG. Thus, we must remove it.
ballsCorr <- ballsAnal |> na.omit()

plt = cor(ballsCorr)
corrplot(plt, method='color', order = 'AOE', type='upper', addCoef.col = 'black')
```

There is a notably high correlation between any higher education rate `EDC` and college graduation rate `EDE`. This is understandable, since any higher education includes a college education. There is also a high correlation between 9th grade education rate `EDA` and high school graduation rate `EDB`. Perhaps this is due to the fact that people who start high school (i.e. 9th grade) will end up finishing it.

### Histograms

We can also look at histograms for each of the variables.

```{r, fig.width=8, fig.height=5}
# declaring vars - b
vars <- c("EDA", "EDB", "EDC", "EDE", "EDG")

bins_fd <- function(x) {
  diff(range(x, na.rm = TRUE)) / (2 * IQR(x, na.rm = TRUE) / length(x)^(1/3))
} #using the Freedman-Diaconis rule to compute the binwidth

his_plots <- map(vars, ~{
  varname <- .x
  binwidth <- round(bins_fd(balls[[varname]]))
  
  ggplot(data=balls, aes_string(x=varname)) + 
    geom_histogram(fill="#2196f3", color="#000000", bins=binwidth) + 
    labs(title=varname, x=NULL, y="Count")
})

wrap_plots(his_plots, ncol = 3, nrow = 2) & plot_annotation(caption="*Note: histogram binwidth was decided using the Freedman-Diaconis rule.")
```

The histograms look quite similar to the ones from the zip code layer. There is skewedness in the 9th grade education rate `EDA`, which makes sense since many people have been to high school. There is also an outlier in the college graduation rate `EDE`, which belongs to census tract 125.03 in Outgamie, WI; they have a college graduation rate of 76.5%.

### Summary Stats

```{r, results='asis'}
stargazer(ballsAnal, title="Summary Stats", type = 'latex', summary.stat = c("n","mean","sd", "min", "median", "max", "p25", "p75"), font.size = "small", notes.append=F, header=F)
```

## Indexing (NEW)

We can try to do an indexing using an arbitrarily decided scale, so we can get indexes from 1-100...

```{r}
condition <- read.csv("Condition-baseline-llct.csv")

kable(condition)
```

Note that the values for the conditions are decided arbitrarily and the values for each of the variables are guesstimated to the best of my ability by cross referencing multiple online sources. The any higher education rate `EDC` variable is especially guesstimated, as almost all of the information I found online was about high school degree+ or bachelor's degree+.

We can then use these baseline values to create a function that maps the values we have for each variable to their condition score. For simplicity's sake, I'm gonna go with a polygonal approximation (sorry Lagrange). The graphs below illustrate the idea.

```{r, fig.height=3, fig.width=15}
vars_cond <- c("EDA", "EDB", "EDC", "EDE", "EDG")

cond_plots <- map(vars_cond, ~{
  cdvar <- .x
  
  ggplot(data=condition, aes_string(x=cdvar, y='Cond_Value')) +
    geom_line() +
    geom_point() +
    labs(title=paste(cdvar, "Score"), x=paste(cdvar, "Value"), y="Condition Score") + scale_x_continuous(limits = c(0,NA))
})

wrap_plots(cond_plots, ncol=5, nrow=1)
```

```{r}
# Code from ChatGPT.. im sorry i cant code... TT

# For each variable, build interpolation function and apply to balls
for (var in vars_cond) {
  
  # Filter condition data for just this variable (non-NA values only)
  baseline_x <- condition[[var]]
  baseline_y <- condition$Cond_Value
  valid_rows <- !is.na(baseline_x) & !is.na(baseline_y)
  
  # Build interpolation function.. approxfun does piecewise linear interpolating
  interp_fun <- approxfun(x = baseline_x[valid_rows], 
                          y = baseline_y[valid_rows], 
                          rule = 2)  # rule = 2 allows extrapolation using nearest value; we need this because some of the values fall below the terrible part, meaning they have a score of 0 (won't show up as na)
  
  # Apply interpolation to balls
  new_colname <- paste0(var, "_CondScore")
  balls[[new_colname]] <- interp_fun(balls[[var]])
}
```

```{r}
# dataset of just the condition scores + index (averaging everything)
indexes <- balls |> select("Name", "GEOID", ends_with("_CondScore")) |> 
  rowwise() |> mutate(Index = mean(c_across(ends_with("_CondScore")), na.rm = TRUE)) |>
  ungroup() |> arrange(desc(Index))

indexes <- indexes |> rename_with(~ gsub("_CondScore.*", "", .x))
```

After some coding that you cannot see, we can display the top 10 and bottom 10 indexes computed using this methodology. The following tables show these condition scores of each of the variables as well as the index, which is an average of those scores.

```{r}
# Top 10 zscores
kable(head(indexes, 10), caption = "Top 10 indexes (by Census Tract)", digits=3)
```

```{r}
# Bottom 10 zscores
kable(tail(indexes, 10), caption = "Bottom 10 indexes (by Census Tract)", digits=3)
```

## Visualization

We can visualize these indexes on a map so we have an intuitive way to compare them.

```{r, fig.width=10, fig.height=10, fig.cap="Map of the lifelong learning index (z-score) by census tract"}
# Making the map for the index computed by averaging the z-scores

condScores <- indexes |> select(GEOID, Index)

# Getting spatial data from tigris, zcta was found from reading the documentation
zips <- unique(condScores$GEOID)
spatial <- tracts(state="wi", class="sf", year=2023, progress_bar=F)
foxZips <- spatial %>% mutate(GEOID=as.character(GEOID)) %>% filter(GEOID %in% zips)
# Getting other zips from tigris so that we know where we are
background <- spatial %>%
  mutate(GEOID=as.character(GEOID)) %>%
  filter(!GEOID %in% zips)

# joining the data tables together
condScores <- condScores %>% mutate(GEOID=as.character(GEOID)) %>%  left_join(foxZips, by=c("GEOID"="GEOID"))

condScores <- condScores %>% st_as_sf()

# Arbitrary limits decided by ME!!! (so, not arbitrary)
zoom_xlim <- c(-89, -88)
zoom_ylim <- c(43.8, 44.7)

# Drawing up the map
cond_map <- ggplot() +
  geom_sf(data = condScores, aes(fill = Index), color="black") +
  geom_sf(data = background, fill = "grey95", color = "grey80", linewidth = 0.1) +
  scale_fill_viridis_c() + 
  labs(title = "Lifelong Learning Index (by Census Tract)") +
  theme_minimal() + coord_sf(xlim = zoom_xlim, ylim = zoom_ylim, expand = FALSE)

cond_map
```

The highest index belongs to tract 31 in Winnebago, while the lowest index belongs to tract 7, also in Winnebago.

I would also like to point out tract 9400, which is on the Oneida Reservation, meaning they have a high population of Native Americans. Namely, the humane housing is the highest, while the lifelong learning index is the second lowest. While many Native Americans attend middle and high school, their drop out rates are disproportionately high, according to the Bureau of Indian Affairs. Post-secondary education graduation rate is even lower, leading to a very low overall lifelong learning index. Indeed, we can see this in the data--only 51.7% of people here have experience in any higher education `EDC`, any only 16.1% have graduated college `EDE`.
