---
title: "FVDE Progress Report 2"
author: Binh, Mina & Dereje
format: pdf
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = F, message=FALSE, warning=FALSE)
options(scipen = 5)
library(tidyverse)
library(stargazer)
library(corrplot)
library(patchwork)
library(tigris)
library(sf)
library(factoextra)
library(kableExtra)
```

# Overview:

This is our second report, incorporating feedback and comments from our 1st meeting and additional team meetings with Professor Sage. Here are the broad strokes of what we've done over the last few weeks:

-   We moved from analyzing zipcodes to census tracts.

-   We replaced the old z-score paradigm with a new 0-100 scoring system that accounted for national averages and trends.

-   We analyzed some of the observed changes and differences when applying our new index scoring paradigm.

# Humane Housing

## Census Tracts Dataset:

We begin with some exploratory data analysis on the census tracts dataset.
```{r}
# Read csv file
hh <- read.csv("hh tracts 2.csv")
hh <- hh %>% select(-Layer, -Name)

# Copying Mina's code for the string pruning process
names(hh) <- sub("_.*$", "", names(hh))

# Creating other dataframes that fit the requirements
hhCorr <- hh %>% select(-HCP) %>% na.omit()
hhSumm <- hh %>% select(-GEOID)
```
### Correlation plots and Histograms:
Below is the correlation plot for numeric variables:
```{r corrplot, fig.width=10, fig.height=10, fig.pos='H'}
A = cor(hhCorr)
corrplot(A, method='color', order = 'AOE', type='upper', addCoef.col = 'black')
```

We can notice the decrease in the number of strong correlations in comparison to the zipcodes dataset. Most variable-pairs now exhibit only some to no correlation at all. Notable correlations that weren't noted in the first report are noted below:

-   `EJV`, the vulnerable demographics index, is negatively correlated with home ownership rates `HUO` and median income `INC`, which is understandable: with higher income, people can afford to buy instead of rent, raising `HUO`, and will not be as vulnerable, resulting in a lower demographic vulnerability index.

-   `FAI`, the metric for food insecurity, shows the same negative correlations with income and home ownership, which is similarly explained.

-   Median household income is also positively correlated with housing cost variables `MHC`, `RNT`, which is expected: the more you earn, the better your housing situation will likely be.

-   Food insecurity and demographic vulnerability also show a strong positive correlation.

```{r, fig.width=10, fig.height=10}
bvars <- names(hhSumm)
hhbins <- function(x) {
  diff(range(x, na.rm = TRUE)) / (2 * IQR(x, na.rm = TRUE) / length(x)^(1/3))
} #using the Freedman-Diaconis rule to compute the binwidth

hhplots <- map(bvars, ~{
  varname <- .x
  binwidth <- round(hhbins(hhSumm[[varname]]))
  
  ggplot(data=hhSumm, aes_string(x=varname)) + 
    geom_histogram(fill="#2196f3", color="#000000", bins=binwidth) + 
    labs(title=varname, x=NULL, y="Count")
})

wrap_plots(hhplots, ncol = 4, nrow = 5) & plot_annotation(caption="*Note: histogram binwidth was decided using the Freedman-Diaconis rule.")
```

The histograms show that even with changing the geographic divisions, most variables show the same trends and distributions. Most variables have a right-tailed distribution, save for `HUO` and `APR`, which are housing ownership rates and mortgage approval rates.

### Summary Statistics:

The table for summary statistics are shown below:

```{r, results='asis'}
stargazer(hhSumm, title="Summary Stats", type = 'latex', summary.stat = c("n","mean","sd", "min", "p25", "median", "p75", "max"), font.size = "small", notes.append=F, header=F)
```

## New Index Creation:

### Variable Selection Process:

Per our discussions last time, our index would contain variables that encompasses affordability, safety, stability, diversity and accessibility. In the time constraint given, I have selected 5 variables that cover most of our bases, which are:

### Variable 1: Housing Costs:

Absolute rent levels mean nothing if we are talking about a national scale - the sole addition of Manhattan's tracts will throw our entire proposed scale off balance. If this is the case, we should probably think of something a little more income-based. Using 2 variables: median housing costs `MHC` and median household income `INC` (from a new hh census tracts file), we can find the housing cost to income ratio.

Some searches reveal that US' best cities tend to have a rent-to-income ratio of \~13% while the worst ones (New York, Miami, Seattle) would be way north of 50%, even 60%. Knowing that MHC is inclusive of mortgages, but also other fees and taxes, I would push this estimate a little to the right: our "ideal" would be any tract with the ratio less than 15%, our "terrible" would be any tract with a ratio of 50% or higher. 30% is proposed as the highest proportion of monthly income the average person should spend on rent, so we would leave it as our "average". That leaves us with 23% as the "good" benchmark and 40% as the "bad" threshold. Below is a table showing the summary statistics of the new rent score:

```{r, results='asis'}
# Creating another column calculating the ratio:
hh <- hh %>% mutate(icPct = MHC/(INC/12)*100, icScore = approx(x=c(15,23,30,40,50), y=c(100,75,50,25,0), xout=icPct, rule=2)$y)

score1 <- hh %>% select(icScore)
stargazer(score1, type = 'latex', summary.stat = c("n","mean","sd", "min", "median", "max"), notes.append=F, header=F)
```

A median value of almost 96 –\> Very right skewed. However, **this tracks with our expectations of Wisconsin/the Midwest in general**. We do have very affordable housing in comparison with other areas in the US.

### Variable 2: Eviction Rates

Eviction rates is also a good proxy for housing availability and stability. However, there are some nuances and considerations to account for working with such a variable:

-   Eviction can mean many things: it can be the filing of the legal process that removes tenants from a property, it can also be the actual removal of such residents from their homes.

-   Some jurisdictions allow landlords to file multiple eviction notices, strongly skewing extreme values (some Maryland counties report a 147% eviction rate, for example).

-   However, the bulk of values (both external and from the dataset) seem to be on the low end, so the thresholds will reflect that.

Currently, the thresholds for ideal/good/average/bad/terrible are 0%/1%/3%/7%/13%. Below is a table showing how the new index applies to our census tract data:

```{r, results='asis'}
hh <- hh %>% mutate(evScore = approx(x=c(0,1,3,7,13), y=c(100,75,50,25,0), xout=EVR, rule=2)$y)
score2 <- hh %>% select(evScore)
stargazer(score2, type = 'latex', summary.stat = c("n","mean","sd", "min", "median", "max"), notes.append=F, header=F)
```

### Variable 3: Home Ownership Rates

As noted by Jason, home ownership rates is a very important metric for housing affordability. There are still some considerations regarding this variable though:

-   Higher `HUO` rates might indicate higher neighborhood stability, but also lack of rental housing.

-   Tracts and areas with transient populations (like dorms and institutional quarters) might have disproportionately low levels of `HUO`, and therefore should be addressed appropriately. I am currently choosing to skip these outliers.

Similar to other variables, the summary statistics table is included below:

```{r, results='asis'}
hh <- hh %>% mutate(hoScore = case_when(
      HUO < 20 ~ NA_real_,
      TRUE ~ approx(
        x    = c(20, 40, 60, 80, 90),
        y    = c(0, 25, 50, 75, 100),
        xout = HUO,
        rule = 2
      )$y))
score3 <- hh %>% select(hoScore)
stargazer(score3, type = 'latex', summary.stat = c("n","mean","sd", "min", "median", "max"), notes.append=F, header=F)
```

### Variable 4: Mean Travel Time

This variable seems to be the most straightforward to create an index from. A quick search confirmed that any commute less than 16 minutes is ideal, and 60+ commutes to be harmful, so we know where to place our thresholds. Here is another table that shows the distribution of our score:

```{r, results='asis'}
hh <- hh %>% mutate(trScore = approx(
        x    = c(16, 20, 25, 35, 60),
        y    = c(100,  75, 50, 25,0),
        xout = TRV,
        rule = 2
      )$y)
score4 <- hh %>% select(trScore)
stargazer(score4, type = 'latex', summary.stat = c("n","mean","sd", "min", "median", "max"), notes.append=F, header=F)
```

### Variable 5: Diversity

The last variable explicitly mentioned by Jason in our search is diversity. Knowing that the highest possible value is 0.875, we should turn to other sources of data and examine the ranges of possible values. The biggest, most diverse cities in the country like New York and LA tend to have a `REX` of 0.75 and higher. Conversely, counties like Laredo, Texas, with a predominantly Hispanic population would only come in at 0.1 or even lower on the scale. Given that this is the case, my current suggestion for the thresholds are 0.15, 0.25, 0.5, 0.65 and 0.75 for conditions ranging from terrible to ideal. Below is a table showing us the distribution of `REX` scores across different tracts of our dataset.

```{r, results='asis'}
hh <- hh %>%
  mutate(diScore = approx(
    x    = c(0.15, 0.25, 0.50, 0.65, 0.75),
    y    = c(0,    25,   50,   75,  100),
    xout = REX,
    rule = 2
  )$y)
score5 <- hh %>% select(diScore)
stargazer(score5, type = 'latex', summary.stat = c("n","mean","sd", "min", "median", "max"), notes.append=F, header=F)
```

### Calculations and Visualizations
With the new index applied to the 5 selected variables, here are graphs that establish the different baselines for each variable (notice the different x-axes for each variable):

```{r, fig.height=4, fig.width=15}
# Define the data for each plot in a list of data frames
plotDataList <- list(
  `Cost / Income Ratio (%)` = tibble(
    Value = c(15, 23, 30, 40, 50),
    Score = c(100, 75, 50, 25, 0)
  ),
  `Eviction Rate (%)` = tibble(
    Value = c(0, 1, 3, 7, 13),
    Score = c(100, 75, 50, 25, 0)
  ),
  `Home Ownership (%)` = tibble(
    Value = c(20, 40, 60, 80, 90),
    Score = c(0, 25, 50, 75, 100)
  ),
  `Commute Time (min)` = tibble(
    Value = c(16, 20, 25, 35, 60),
    Score = c(100, 75, 50, 25, 0)
  ),
  `Diversity Index` = tibble(
    Value = c(0.15, 0.25, 0.50, 0.65, 0.75),
    Score = c(0, 25, 50, 75, 100)
))

# Use imap to loop through the list, creating a plot for each item.
scorePlots <- imap(plotDataList, ~ {
  ggplot(data = .x, aes(x = Value, y = Score)) +
    geom_line() +
    geom_point(size = 2.5) +
    labs(title = .y, x = "Variable Value", y = "Condition Score") +
    theme_bw() +
    scale_y_continuous(limits = c(0, 100), breaks = c(0, 25, 50, 75, 100)) +
    theme(strip.background = element_blank())
})

# Combine the individual plots into a single row.
wrap_plots(scorePlots, ncol = 5)
```

The current index adapts the proposed weighting scheme to our five selected variables. While other relevant metrics exist in the dataset (like food insecurity), they were pruned for the sake of time and brevity. Future reports will incorporate these additional factors to create a more robust model. For this report only, the weights for the variables are:

1 \* Income/Rent, 1 \* Home Ownership, 0.5 \* Eviction Rate, 0.5 \* Commute Time, 0.5 \* Diversity.

This total is then divided by 3.5 to get a humane housing index going from 0 to 100.

\newpage

## New Index Analysis:

With our 5 variables selected, here is a map containing the new humane housing score for each census tracts and a table containing the top and bottom 10 tracts along with individual scores:

```{r, fig.width=10, fig.height=10}
# Arbitrary limits decided by Mina (so, not arbitrary)
zoom_xlim <- c(-89, -88)
zoom_ylim <- c(43.8, 44.7)

ind <- hh %>%  select(GEOID, ends_with("Score")) %>% mutate(newIndex = (icScore + hoScore + (0.50 * trScore) + (0.50 * diScore) + (0.50 * evScore)) / 3.50)  %>%  arrange(desc(newIndex))

# Getting spatial data from tigris, zcta was found from reading the documentation
zips <- unique(hh$GEOID)
spatial <- tracts(state="wi", class="sf", year=2023, progress_bar=F)
foxZips <- spatial %>% mutate(GEOID=as.character(GEOID)) %>% filter(GEOID %in% zips)
# Getting other zips from tigris so that we know where we are
background <- spatial %>%
  mutate(GEOID=as.character(GEOID)) %>%
  filter(!GEOID %in% zips)

# joining the data tables together
ind <- ind %>% mutate(GEOID=as.character(GEOID)) %>%  left_join(foxZips, by=c("GEOID"="GEOID"))

ind <- ind %>% st_as_sf()


# Drawing up the map
ggplot() +
  geom_sf(data = ind, aes(fill = newIndex), color="black") +
  geom_sf(data = background, fill = "grey95", color = "grey80", linewidth = 0.1) +
  scale_fill_viridis_c() + 
  labs(title = "Map of Census Tracts by New Housing Index") +
  theme_minimal() + coord_sf(xlim = zoom_xlim, ylim = zoom_ylim, expand = FALSE)
```

\newpage

```{r}
# Drop geometry, rename columns, keep only the scores
scoreTbl <- ind %>% 
  st_drop_geometry() %>% 
  rename(
    Tract           = GEOID,
    CostIncome = icScore,
    Eviction   = evScore,
    Ownership  = hoScore,
    Commute   = trScore,
    Diversity  = diScore,
    Overall    = newIndex
  ) %>% 
  select(Tract, CostIncome, Eviction, Ownership,
         Commute, Diversity, Overall)

# Slice out top-10 and bottom-10
top10    <- scoreTbl %>% arrange(desc(Overall)) %>% slice_head(n = 10)
bottom10 <- scoreTbl %>% arrange(Overall)        %>% slice_head(n = 10)

# Render with kable, putting most args on one line
knitr::kable(top10,    caption = "Top 10 Tracts",    digits = 1, booktabs = TRUE) %>% 
  kable_styling(full_width = FALSE)


knitr::kable(bottom10, caption = "Bottom 10 Tracts", digits = 1, booktabs = TRUE) %>% 
  kable_styling(full_width = FALSE)
```

There are some observations about the best and worst humane housing performers we can make:

-   The census tract with the highest humane housing index is tract 9400, which is on the Oneida reservation. It led other census tracts not only in terms of affordability but also diversity, which is the reason why it got the highest average score.

-   Looking at other tracts in the top 10 table, we can see that they are performing well not because of, but in spite of the diversity score: 5/10 tracts have a diversity index of less than 12/100. This is expected for small areas in a state like Wisconsin, potentially meriting some further investigation into the best way to account for diversity.

-   As for census tracts in the bottom 10, a consistent trend among these areas is low home ownership scores. Though the metric can be distorted by the presence of institution quarters and dorms, they also imply housing instability and understandably less humane housing outcomes.

-   There are also tracts in the bottom 10 which are designated opportunity zones - less fortunate areas which have received federal assistance and funding.


# Lifelong Learning

We look into lifelong learning for the census tract subdivision.

```{r}
#file orwhatever
# getting the data into readable format
colnames <- names(read.csv("Lifelong-Learning-Cencus-Tracts.csv",nrows=0))

# data wrangling blah blah blah
balls <- read.csv("Lifelong-Learning-Cencus-Tracts.csv", skip=2, header=FALSE)
names(balls) <- colnames
balls <- balls |> select(-Layer) # removing redundant columns
names(balls)[4:8] <- c("EDA", "EDB", "EDC", "EDE", "EDG") # get rid of the stupid date things since they're all the same (see below)

# data for the prelim analysis
ballsAnal <- balls |> select(-GEOID, -Name)
```

## Legend

Similar to the zip code level, the census tract variable names do not provide much insight into what they are representing. Thus, I will describe them here. Please note that all of this data was collected from 2019-2023.

-   `EDA`: 9th grade education rate (% of residents)
-   `EDB`: High school graduation rate (% of residents)
-   `EDC`: Any higher education rate (% of residents)
-   `EDE`: College graduation rate (% of residents)
-   `EDG`: Preschool enrollment rate (% of toddlers ages 3-4)

Good news! There are basically no `NA` entries in this dataset at all, so we do not need to worry about that!! The one `NA` entry is in the preschool enrollment rate `EDG` in census tract 7. Also worth noting that there are a lot of 0 entries in that column. Not sure why that is.

## Exploratory Plots and Variables

### Correlation Plots

```{r, fig.width=5, fig.height=5}
# there is one NA entry in EDG. Thus, we must remove it.
ballsCorr <- ballsAnal |> na.omit()

plt = cor(ballsCorr)
corrplot(plt, method='color', order = 'AOE', type='upper', addCoef.col = 'black')
```

There is a notably high correlation between any higher education rate `EDC` and college graduation rate `EDE`. This is understandable, since any higher education includes a college education. There is also a high correlation between 9th grade education rate `EDA` and high school graduation rate `EDB`. Perhaps this is due to the fact that people who start high school (i.e. 9th grade) will end up finishing it.

### Histograms

We can also look at histograms for each of the variables.

```{r, fig.width=8, fig.height=5}
# declaring vars - b
vars <- c("EDA", "EDB", "EDC", "EDE", "EDG")

bins_fd <- function(x) {
  diff(range(x, na.rm = TRUE)) / (2 * IQR(x, na.rm = TRUE) / length(x)^(1/3))
} #using the Freedman-Diaconis rule to compute the binwidth

his_plots <- map(vars, ~{
  varname <- .x
  binwidth <- round(bins_fd(balls[[varname]]))
  
  ggplot(data=balls, aes_string(x=varname)) + 
    geom_histogram(fill="#2196f3", color="#000000", bins=binwidth) + 
    labs(title=varname, x=NULL, y="Count")
})

wrap_plots(his_plots, ncol = 3, nrow = 2) & plot_annotation(caption="*Note: histogram binwidth was decided using the Freedman-Diaconis rule.")
```

The histograms look quite similar to the ones from the zip code layer. There is skewedness in the 9th grade education rate `EDA`, which makes sense since many people have been to high school. There is also an outlier in the college graduation rate `EDE`, which belongs to census tract 125.03 in Outgamie, WI; they have a college graduation rate of 76.5%.

### Summary Stats

```{r, results='asis'}
stargazer(ballsAnal, title="Summary Stats", type = 'latex', summary.stat = c("n","mean","sd", "min", "median", "max", "p25", "p75"), font.size = "small", notes.append=F, header=F)
```

## Indexing (NEW)

We can try to do an indexing using an arbitrarily decided scale, so we can get indexes from 1-100...

```{r}
condition <- read.csv("Condition-baseline-llct.csv")

kable(condition)
```

Note that the values for the conditions are decided arbitrarily and the values for each of the variables are guesstimated to the best of my ability by cross referencing multiple online sources. The any higher education rate `EDC` variable is especially guesstimated, as almost all of the information I found online was about high school degree+ or bachelor's degree+.

We can then use these baseline values to create a function that maps the values we have for each variable to their condition score. For simplicity's sake, I'm gonna go with a polygonal approximation (sorry Lagrange). The graphs below illustrate the idea.

```{r, fig.height=3, fig.width=15}
vars_cond <- c("EDA", "EDB", "EDC", "EDE", "EDG")

cond_plots <- map(vars_cond, ~{
  cdvar <- .x
  
  ggplot(data=condition, aes_string(x=cdvar, y='Cond_Value')) +
    geom_line() +
    geom_point() +
    labs(title=paste(cdvar, "Score"), x=paste(cdvar, "Value"), y="Condition Score") + scale_x_continuous(limits = c(0,NA))
})

wrap_plots(cond_plots, ncol=5, nrow=1)
```

```{r}
# Code from ChatGPT.. im sorry i cant code... TT

# For each variable, build interpolation function and apply to balls
for (var in vars_cond) {
  
  # Filter condition data for just this variable (non-NA values only)
  baseline_x <- condition[[var]]
  baseline_y <- condition$Cond_Value
  valid_rows <- !is.na(baseline_x) & !is.na(baseline_y)
  
  # Build interpolation function.. approxfun does piecewise linear interpolating
  interp_fun <- approxfun(x = baseline_x[valid_rows], 
                          y = baseline_y[valid_rows], 
                          rule = 2)  # rule = 2 allows extrapolation using nearest value; we need this because some of the values fall below the terrible part, meaning they have a score of 0 (won't show up as na)
  
  # Apply interpolation to balls
  new_colname <- paste0(var, "_CondScore")
  balls[[new_colname]] <- interp_fun(balls[[var]])
}
```

```{r}
# dataset of just the condition scores + index (averaging everything)
indexes <- balls |> select("Name", "GEOID", ends_with("_CondScore")) |> 
  rowwise() |> mutate(Index = mean(c_across(ends_with("_CondScore")), na.rm = TRUE)) |>
  ungroup() |> arrange(desc(Index))

indexes <- indexes |> rename_with(~ gsub("_CondScore.*", "", .x))
```

After some coding that you cannot see, we can display the top 10 and bottom 10 indexes computed using this methodology. The following tables show these condition scores of each of the variables as well as the index, which is an average of those scores.

```{r}
# Top 10 zscores
kable(head(indexes, 10), caption = "Top 10 indexes (by Census Tract)", digits=3)
```

```{r}
# Bottom 10 zscores
kable(tail(indexes, 10), caption = "Bottom 10 indexes (by Census Tract)", digits=3)
```

## Visualization

We can visualize these indexes on a map so we have an intuitive way to compare them.

```{r, fig.width=10, fig.height=10, fig.cap="Map of the lifelong learning index (z-score) by census tract"}
# Making the map for the index computed by averaging the z-scores

condScores <- indexes |> select(GEOID, Index)

# Getting spatial data from tigris, zcta was found from reading the documentation
zips <- unique(condScores$GEOID)
spatial <- tracts(state="wi", class="sf", year=2023, progress_bar=F)
foxZips <- spatial %>% mutate(GEOID=as.character(GEOID)) %>% filter(GEOID %in% zips)
# Getting other zips from tigris so that we know where we are
background <- spatial %>%
  mutate(GEOID=as.character(GEOID)) %>%
  filter(!GEOID %in% zips)

# joining the data tables together
condScores <- condScores %>% mutate(GEOID=as.character(GEOID)) %>%  left_join(foxZips, by=c("GEOID"="GEOID"))

condScores <- condScores %>% st_as_sf()

# Arbitrary limits decided by ME!!! (so, not arbitrary)
zoom_xlim <- c(-89, -88)
zoom_ylim <- c(43.8, 44.7)

# Drawing up the map
cond_map <- ggplot() +
  geom_sf(data = condScores, aes(fill = Index), color="black") +
  geom_sf(data = background, fill = "grey95", color = "grey80", linewidth = 0.1) +
  scale_fill_viridis_c() + 
  labs(title = "Lifelong Learning Index (by Census Tract)") +
  theme_minimal() + coord_sf(xlim = zoom_xlim, ylim = zoom_ylim, expand = FALSE)

cond_map
```

The highest index belongs to tract 31 in Winnebago, while the lowest index belongs to tract 7, also in Winnebago.

I would also like to point out tract 9400, which is on the Oneida Reservation, meaning they have a high population of Native Americans. Namely, the humane housing is the highest, while the lifelong learning index is the second lowest. While many Native Americans attend middle and high school, their drop out rates are disproportionately high, according to the Bureau of Indian Affairs. Post-secondary education graduation rate is even lower, leading to a very low overall lifelong learning index. Indeed, we can see this in the data--only 51.7% of people here have experience in any higher education `EDC`, any only 16.1% have graduated college `EDE`.

# Reliable Transportation

## Exploratory Data Analysis:

Here is the list of Variables that we will be working with and their definitions.

-   `ACT` This is the percentage of workers who walk or bike to work
-   `CAR` This is the percentage of workers who drive alone to work
-   `PUB` This is the percentage of workers using public transit
-   `NVC` This is the percentage of households without a vehicle
-   `TRV` This is the mean travel time to work.
-   `EKW` This is the walkability index, where higher values indicate higher walkability.

```{r}
# Function to take the column names
colnames <- names(read.csv("TransportationCensusTracts2.csv", nrows = 0))

# Read the data, skipping the first two rows, then re-assign names
RelTransport <- read.csv("TransportationCensusTracts2.csv", skip = 5, header = F)
names(RelTransport) <- colnames
RelTransport <- RelTransport %>% select(-Layer, -Name)

# Copying Mina's code for the string pruning process
names(RelTransport) <- sub("_.*$", "", names(RelTransport))

# Creating other dataframes that fit the requirements
RelTransportCorr <- RelTransport %>% select(-GEOID, -Longitude, -Latitude) %>% na.omit() 
RelTransportSumm <- RelTransport %>% select(-GEOID, -Longitude, -Latitude) 
```

### Correlation Plot:

```{r, fig.width=5, fig.height=5}

A = cor(RelTransportCorr)
corrplot(A, method='color', order = 'AOE', type='upper', addCoef.col = 'black')
```

ACT (walk/bike) and NVC (no vehicle access) show a strong positive correlation, suggesting areas with more active commuting also have more households without cars. Walkability (EKW) is moderately to strongly correlated with ACT, NVC, and PUB, supporting the idea that more walkable areas promote alternative transportation modes. TRV (travel time) is negatively correlated with EKW and PUB, indicating that walkable and transit-accessible areas tend to have shorter commutes. CAR (drive alone) remains negatively correlated with ACT, PUB, and NVC, but these relationships are somewhat weaker than in the ZIP-level data, possibly due to more localized variation. Overall, the matrix highlights similar trends as the ZIP-level report but with sharper contrasts.

### Histograms

```{r, fig.width=12, fig.height=8}
RelTransportvars <- c(
  "ACT",
  "CAR",
  "EKW",
  "NVC",
  "PUB",
  "TRV"
)

RelTransportbins <- function(x) {
  diff(range(x, na.rm = TRUE)) / (2 * IQR(x, na.rm = TRUE) / length(x)^(1/3))
} #using the Freedman-Diaconis rule to compute the binwidth

RelTransportplots <- map(RelTransportvars, ~{
  varname <- .x
  binwidth <- round(RelTransportbins(RelTransportSumm[[varname]]))
  
  ggplot(data=RelTransportSumm, aes_string(x=varname)) + 
    geom_histogram(fill="#2196f3", color="#000000", bins=binwidth) + 
    labs(title=varname, x=NULL, y="Count")
})

wrap_plots(RelTransportplots, ncol = 3, nrow = 2) 
```

ACT, PUB, and NVC are all strongly right-skewed, indicating that most census tracts have low levels of walking/biking, transit use, and households without vehicles, with only a few areas showing high values. CAR remains tightly distributed and slightly left-skewed, reflecting that driving alone is still the dominant commuting mode in most tracts. TRV shows a moderate right skew, meaning most commute times are on the shorter side, but a few tracts have longer travel times. EKW (walkability) appears more balanced, roughly normal, with a slight right skew—most tracts have moderate walkability, but some urban cores are significantly more walkable. As with the ZIP-level results, highly skewed variables like PUB may contribute noise in rural tracts and might benefit from transformation or adjusted weighting in the index.

### Summary Statistics

```{r, results='asis'}
# 1. Compute the summary table
statTbl <- RelTransportSumm %>%
  summarise(across(
    everything(),
    list(
      N      = ~sum(!is.na(.)),
      Mean   = ~mean(.,   na.rm = TRUE),
      StDev  = ~sd(.,     na.rm = TRUE),
      Min    = ~min(.,    na.rm = TRUE),
      Q1     = ~quantile(., 0.25, na.rm = TRUE),
      Median = ~median(., na.rm = TRUE),
      Q3     = ~quantile(., 0.75, na.rm = TRUE),
      Max    = ~max(.,    na.rm = TRUE),
      PctNA  = ~mean(is.na(.)) * 100
    ),
    .names = "{.col}_{.fn}"
  )) %>%
  pivot_longer(
    cols          = everything(),
    names_to      = c("Variable","Statistic"),
    names_pattern = "(.+)_(.+)"
  ) %>%
  pivot_wider(
    names_from  = Statistic,
    values_from = value
  ) %>%
  relocate(Variable, N, PctNA, Mean, StDev, Min, Q1, Median, Q3, Max) %>%
  mutate(
    N     = as.integer(N),
    PctNA = round(PctNA, 1),
    across(c(Mean, StDev, Min, Q1, Median, Q3, Max), ~ round(., 3))
  )

# 2. Print it with kableExtra
statTbl %>%
  kbl(
    format    = "latex",
    booktabs  = TRUE,
    caption   = "Summary Statistics (PctNA = \\% missing)",
    label     = "tab:summary",
    align     = c("l", rep("r", 9)),
    digits    = 3
  ) %>%
  kable_styling(
    latex_options   = c("scale_down","hold_position"),
    font_size       = 8,
    full_width      = T
  )
```

## New Indexing System

```{r}
# Function to take the column names
colnames2 <- names(read.csv("TransportationCensusTracts2.csv", nrows = 0))

# Read the data, skipping the first two rows, then re-assign names
RelTransport <- read.csv("TransportationCensusTracts2.csv", skip = 5, header = F)
names(RelTransport) <- colnames2
RelTransport <- RelTransport %>% select(-Layer, -Name)

# Copying Mina's code for the string pruning process
names(RelTransport) <- sub("_.*$", "", names(RelTransport))

# Creating other dataframes that fit the requirements
RelTransportCorr2 <- RelTransport %>% select(-GEOID, -Longitude, -Latitude) %>% na.omit() 
RelTransportSumm2 <- RelTransport %>% select(-GEOID, -Longitude, -Latitude) 
```

For this next section I will apply the new variable scoring method discussed in our last meeting to the topic of reliable transportation. The approach involves assigning scores based on thresholds I've chosen after some research. Scores will be scaled from 0 to 100, where 0 reflects the least favorable transportation conditions and 100 reflects the most favorable, based on national comparisons. Thresholds for categories like “ideal,” “good,” “average,” “bad,” and “terrible” are chosen using a combination of outside research, logical reasoning, and the shape of each variable’s distribution.

I will beging this process for each of the 6 Transportation related variables and explain the process along the way. 


### Variable 1: ACT

This is the percentage of workers who walk or bike to work

#### National Benchmarks for Walking/Biking to Work

Here is some information about average walking and biking-to-work rates in the United States from the [League of American Bicyclists](https://data.bikeleague.org/data/national-rates-of-biking-and-walking/). According to this article, based on American Community Survey (ACS) data, approximately 2.4% of people walk to work and 0.5% bike to work on average as of 2023. The ACS began reporting commute-to-work estimates in 2005, with more consistent data available since 2010.
Additional data from [Census.gov](https://www.census.gov/library/stories/2019/05/younger-workers-in-cities-more-likely-to-bike-to-work.html) notes that several cities with high biking rates tend to have large university populations. For example, in Davis, California, nearly 20% of workers commute by bicycle, and in Boulder, Colorado, the rate is 10.4%.
A separate article from [Census.gov](https://www.census.gov/newsroom/blogs/random-samplings/2014/05/where-do-people-bike-walk-to-work-college-towns.html) focuses more on walking, reporting that in Ithaca, New York—home to Cornell University—more than 42% of commuters walk to work. Cambridge, Massachusetts, which includes Harvard and MIT, sees 24% of its workers commuting on foot. These examples highlight how city type and population characteristics influence walk/bike rates.

To translate the ACT variable into a 0–100 score, we can select five threshold values based on the observed distribution. Based on our Dataset, A score of 0 is assigned to census tracts where no workers walk or bike to work, while 1% is considered poor (score of 25). The midpoint score of 50 is set at 3.5%, which closely matches the dataset’s mean. A value of 7% reflects a relatively high rate and is scored as 75. Finally, 15% or above is considered ideal. These would be the points based on the distibution of our data, but if we compare with national benchmarks we can adjust these points accordingly.

After reviewing national averages, I made several adjustments to align our scoring more closely with real-world commuting trends. Nationally, around 2.9% of people walk or bike to work combined, with rates above 10–20% typically found only in exceptional cities like Davis or Ithaca.

Therefore, I updated the benchmarks as follows:

0% remains the lower bound and receives a score of 0.

1.2%, below the national average, is now the threshold for “bad” (score = 25).

3.5% represents average conditions and is set as “average” (score = 50).

7%, which is much higher than national average, is considered “good” (score = 75).

17%, based on high-performing outliers like Davis, CA, is set as “ideal” (score = 100).

```{r results='asis'}
# Score for ACT: % of workers who walk or bike to work
RelTransport <- RelTransport %>%
  mutate(actScore = approx(
    x = c(0, 1.2, 3.5, 7, 17),         # subjective cutoffs based on spread
    y = c(0, 25, 50, 75, 100),       # corresponding scores
    xout = ACT,
    rule = 2                         # clamp values outside the range
  )$y)

# Summary table for actScore
score1 <- RelTransport %>% select(actScore)
stargazer(score1, type = 'latex', summary.stat = c("n", "mean", "sd", "min", "median", "max"), notes.append = F, header = F)

```

### Variable 2: CAR

This is the percentage of workers who drive alone to work

#### Compare to National Rates

According to data from the U.S. Census Bureau, the average rate of people who drove alone to work in 2019 was approximately 76%. This rate dropped to around 68% in 2022 due to the COVID-19 pandemic and increased remote work. For the purpose of this scoring system, we will use 76% as the national benchmark for "average" commuting behavior.

Driving alone tends to be more common in suburban and rural areas, where public transit, walkability, and other transportation alternatives are limited. Therefore, higher values for this variable generally reflect lower transportation diversity and less sustainable commuting options.

Although this may be subjective, lets say that high rates of driving alone are undesirable. Therefore, the scoring scale is reversed: lower values receive higher scores, and higher values receive lower scores.

Based on our dataset, which ranges from about 44% to 91%, and the national benchmark of 76%, we chose the following breakpoints:

90% or more is considered terrible (score = 0) 82% reflects bad conditions (score = 25) 76%, the national average, is set as average (score = 50) 65% is considered good (score = 75) 50% or below reflects ideal conditions (score = 100)

```{r results='asis'}
# Score for CAR: % of workers who drive alone to work
RelTransport <- RelTransport %>%
  mutate(carScore = approx(
    x = c(90, 82, 76, 65, 50),   # reversed order because lower CAR is better
    y = c(0, 25, 50, 75, 100),   # corresponding scores
    xout = CAR,
    rule = 2                     # clamp values outside the range
  )$y)

# Summary table for carScore
score2 <- RelTransport %>% select(carScore)
stargazer(score2, type = 'latex', summary.stat = c("n", "mean", "sd", "min", "median", "max"), notes.append = FALSE, header = FALSE)
```

### Variable 3: PUB

This is the percentage of workers using public transit

#### National Rates:

According to the [U.S. Census Bureau](https://www.census.gov/content/dam/Census/library/publications/2021/acs/acs-48.pdf), around 5% of U.S. workers used public transportation to get to work as of the most recent American Community Survey (ACS) data. This average masks significant regional variation:
In the Northeastern U.S., public transit usage is higher, with an overall average of 14.3%, and up to 35% in metro areas (principal cities). In contrast, the Southern U.S. shows much lower averages—around 2% overall, and as low as 0.3% outside metro areas.

Higher percentages of public transit use are considered more favorable for transportation equity, sustainability, and reliability. Therefore, the scoring scale increases with increasing values.

Our dataset has a mean of 0.56%, a median of 0%, and a maximum of only 7.5%, which is far below national averages. This suggests that most tracts in our sample rely heavily on private vehicles, and that even a modest amount of transit use is above average in this context.

Using both the national context and our local distribution, we set the following thresholds:

0% = terrible (score = 0) 0.5% = bad (score = 25) — roughly aligns with our dataset's mean 2% = average (score = 50) — below national average but high for our data 5% = good (score = 75) — national average 10% = ideal (score = 100) — rare but achievable in well-served urban areas

```{r results='asis'}

# Score for PUB: % of workers using public transit
RelTransport <- RelTransport %>%
  mutate(pubScore = approx(
    x = c(0, 0.5, 2, 5, 10),
    y = c(0, 25, 50, 75, 100),
    xout = PUB,
    rule = 2
  )$y)

# Summary table for pubScore
score3 <- RelTransport %>% select(pubScore)
stargazer(score3, type = 'latex', summary.stat = c("n", "mean", "sd", "min", "median", "max"), notes.append = FALSE, header = FALSE)
```

### Variable 4: NVC

This is the percentage of households without a vehicle

#### National Rates for Vehicle Access

According to U.S. Census Bureau data, approximately 8.5% of households in the U.S. had no vehicle available as of the 2020 American Community Survey (ACS) 5-Year Estimates.

There is substantial variation across regions and cities:

In dense urban areas like New York City, up to 55% of households do not have a vehicle. In contrast, rural and suburban regions may have 1–2% or fewer zero-vehicle households. The national median across tracts is much lower than 8.5% due to the geographic spread of car-dependent areas. Deciding weather higher or lower values are favourable for this variable is dificult. High percentage of households without cars may reflect either poverty or strong public transit infrastructure. Because we're scoring this in the context of reliable transportation access, lets say that higher percentages without a vehicle is less desirable especially for areas with poor transit options.

In this context, higher NVC values are considered worse, so the scoring scale is inverted: lower values are better.

In our dataset: Mean = 4.78% Median = 3.20% Max = 27.04% Based on both this and national ranges, we define:

30%+ = terrible (score = 0) — very high percentage with no vehicle 15% = bad (score = 25) 8.5% = average (score = 50) — matches national average 4% = good (score = 75) — slightly below our dataset’s mean 1% or less = ideal (score = 100)

```{r results='asis'}
# Score for NVC: % of households without a vehicle (lower is better)
RelTransport <- RelTransport %>%
  mutate(nvcScore = approx(
    x = c(25, 15, 8.5, 4, 0),   # descending because lower is better
    y = c(0, 25, 50, 75, 100),  # corresponding scores
    xout = NVC,
    rule = 2
  )$y)

# Summary table for nvcScore
score4 <- RelTransport %>% select(nvcScore)
stargazer(score4, type = 'latex', summary.stat = c("n", "mean", "sd", "min", "median", "max"), notes.append = FALSE, header = FALSE)
```

### Variable 5: TRV

This is the mean travel time to work.

This variable should be easier to interpret, as shorter travel times to work are considered favorable in most circumstances.

#### National Rates for Commute Time

According to the U.S. Census Bureau’s 2021 ACS 1-Year Estimates, the national average commute time in the United States was approximately 26.4 minutes. Commute times vary by geography:

In large metro areas (e.g., New York, Washington D.C., Los Angeles), commute times often exceed 35 minutes. In smaller cities and rural areas, commutes can be as short as 10–15 minutes.

Because shorter commutes are preferred, the scoring system is inversely scaled—lower values receive higher scores.

Our dataset ranges from 11.8 to 31.7 minutes, with a mean of \~19.9 and a median of \~19.1, which is well below the national average. This suggests our study area experiences relatively short commute times overall.

We selected the following thresholds for scoring:

35 minutes or more = terrible (score = 0) 30 minutes = bad (score = 25) 26.4 minutes = average (score = 50) — matches national average 20 minutes = good (score = 75) — slightly above our local median 15 minutes or less = ideal (score = 100)

```{r results='asis'}

# Score for TRV: mean travel time to work (lower is better)
RelTransport <- RelTransport %>%
  mutate(trvScore = approx(
    x = c(35, 30, 26.4, 20, 15),
    y = c(0, 25, 50, 75, 100),
    xout = TRV,
    rule = 2
  )$y)


# Summary table for trvScore
score5 <- RelTransport %>% select(trvScore)
stargazer(score5, type = 'latex', summary.stat = c("n", "mean", "sd", "min", "median", "max"), notes.append = FALSE, header = FALSE)
```

### Variable 6: EKW

This is the walkability index, where higher values indicate higher walkability.

#### National rates for Walkability Index

According to the Fox Valley Data Exchange, the EKW variable represents a walkability ranking based on intersection density, proximity to transit, diversity of nearby businesses, and housing density. It is sourced from the Environmental Justice Index, developed by the Agency for Toxic Substances and Disease Registry (ATSDR).

Values range from 1 to 20, with 20 representing the most walkable environments—typically dense, mixed-use urban areas—and 1 representing very car-dependent, low-connectivity zones.

Our dataset ranges from 3.67 to 15.92, with a mean of \~8.57 and a median of \~7.99, suggesting that most census tracts fall in the moderate range of walkability. Tracts near or above 15 are strong outliers and likely represent downtown cores or neighborhoods with compact, transit-rich design.

Since higher walkability is a favorable outcome, the scoring increases with EKW.

Based on both the dataset and the 1–20 scale from ATSDR, the following breakpoints were chosen:

3 or below = terrible (score = 0) — very limited walkability 6 = bad (score = 25) — below dataset median 9 = average (score = 50) — near dataset mean 13 = good (score = 75) 16 or above = ideal (score = 100) — strong walkable tracts, close to scale maximum

```{r results='asis'}
# Score for EKW: Walkability index (higher is better)
RelTransport <- RelTransport %>%
  mutate(ekwScore = approx(
    x = c(3, 6, 9, 13, 16),
    y = c(0, 25, 50, 75, 100),
    xout = EKW,
    rule = 2
  )$y)

# Summary table for ekwScore
score6 <- RelTransport %>% select(ekwScore)
stargazer(score6, type = 'latex', summary.stat = c("n", "mean", "sd", "min", "median", "max"), notes.append = FALSE, header = FALSE)
```

These graphs show how each transportation variable is converted into a standardized score from 0 to 100. The curves reflect how better or worse values are weighted when calculating the overall transportation index.
```{r, fig.height=4, fig.width=10}
library(tibble)
library(purrr)
library(ggplot2)
library(patchwork)

# Define scoring curves with renamed internal variables
transportScoringCurves <- list(
  `ACT-Walk/Bike to Work` = tibble(
    act_vals = c(0, 1.2, 3.5, 7, 17),
    act_scores = c(0, 25, 50, 75, 100)
  ),
  `CAR-Drive Alone to Work` = tibble(
    car_vals = c(90, 82, 76, 65, 50),
    car_scores = c(0, 25, 50, 75, 100)
  ),
  `PUB-Uses Public Transit` = tibble(
    pub_vals = c(0, 0.5, 2, 5, 10),
    pub_scores = c(0, 25, 50, 75, 100)
  ),
  `NVC-Housholds Without Vehicle` = tibble(
    nvc_vals = c(25, 15, 8.5, 4, 0),
    nvc_scores = c(0, 25, 50, 75, 100)
  ),
  `TRV-Travel Time to Work` = tibble(
    trv_vals = c(35, 30, 26.4, 20, 15),
    trv_scores = c(0, 25, 50, 75, 100)
  ),
`EKW-Walkability Index` = tibble(
    ekw_vals = c(3, 6, 9, 13, 16),
    ekw_scores = c(0, 25, 50, 75, 100)
  )
)

# Plots
scoringPlots <- imap(transportScoringCurves, ~ {
  ggplot(.x, aes(x = .x[[1]], y = .x[[2]])) +
    geom_line() +
    geom_point(size = 2.5) +
    labs(title = .y, x = "Value", y = "Condition Score") +
    theme_bw() +
    scale_y_continuous(limits = c(0, 100), breaks = c(0, 25, 50, 75, 100)) +
    theme(strip.background = element_blank())
})

# Wrap plots into 2 rows of 3 for better fit
wrap_plots(scoringPlots, ncol = 3)

```
## Map using new Index
```{r, fig.width=10, fig.height=10}


options(tigris_use_cache = TRUE)

zoom_xlim <- c(-89, -88)
zoom_ylim <- c(43.8, 44.7)

# Get spatial data
spatial <- tracts(state = "WI", class = "sf", year = 2023, progress_bar = FALSE)
zips <- unique(RelTransport$GEOID)

foxZips <- spatial %>% mutate(GEOID = as.character(GEOID)) %>% filter(GEOID %in% zips)
background <- spatial %>% mutate(GEOID = as.character(GEOID)) %>% filter(!GEOID %in% zips)

# Compute the new index
ind <- RelTransport %>%
  select(GEOID, ends_with("Score")) %>%
  rowwise() %>%
  mutate(newIndex = mean(c_across(ends_with("Score")), na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(GEOID = as.character(GEOID)) %>%
  left_join(foxZips, by = "GEOID") %>%
  st_as_sf()

# Map
ggplot() +
  geom_sf(data = ind, aes(fill = newIndex), color = "black") +
  geom_sf(data = background, fill = "grey95", color = "grey80", linewidth = 0.1) +
  scale_fill_viridis_c(name = "Index", limits = c(0, 100)) +
  labs(title = "Map of Census Tracts by Transportation Index") +
  theme_minimal() +
  coord_sf(xlim = zoom_xlim, ylim = zoom_ylim, expand = FALSE)

```


I set 0-100 limits on the scale for this map despite values in our dataset not reaching both ends of the spectrum. This is helpful so that we can compare the tri county to national benchmarks and understand where we stand relative to other areas in the United States.

```{r, fig.width=10, fig.height=10, fig.cap="Map of Reliable Transportation index by census tract"}
# library(tigris)
# library(sf)
# library(ggplot2)
# library(dplyr)
# library(viridis)
# 
# options(tigris_use_cache = TRUE)
# 
# # Set zoom limits
# zoom_xlim <- c(-89, -88)
# zoom_ylim <- c(43.8, 44.7)
# 
# # Pull tracts from WI
# spatial <- tracts(state = "WI", class = "sf", year = 2023, progress_bar = FALSE)
# 
# # Identify GEOIDs we care about
# zips <- unique(RelTransport$GEOID)
# 
# # Subset spatial layers
# foxZips <- spatial %>% mutate(GEOID = as.character(GEOID)) %>% filter(GEOID %in% zips)
# background <- spatial %>% mutate(GEOID = as.character(GEOID)) %>% filter(!GEOID %in% zips)
# 
# # Build the new scoring index
# ind <- RelTransport %>%
#   select(GEOID, ends_with("Score")) %>%
#   rowwise() %>%
#   mutate(newIndex = mean(c_across(ends_with("Score")), na.rm = TRUE)) %>%
#   ungroup() %>%
#   mutate(GEOID = as.character(GEOID)) %>%
#   st_drop_geometry() %>%  # drop geometry in case it's an sf already
#   left_join(foxZips, by = "GEOID") %>%
#   st_as_sf()
# 
# # COUNTY OUTLINES
# county_outline <- counties(state = "WI", year = 2023, class = "sf") %>%
#   filter(NAME %in% c("Outagamie", "Winnebago", "Calumet"))
# 
# # LAKES
# lakes <- rbind(
#   area_water(state = "WI", county = "Outagamie", year = 2023),
#   area_water(state = "WI", county = "Winnebago", year = 2023),
#   area_water(state = "WI", county = "Calumet", year = 2023)
# )
# 
# # Final map using newIndex but with geographic features
# ggplot() +
#   geom_sf(data = county_outline, fill = "grey80", color = "white", linewidth = 0.2) +     # county background
#   geom_sf(data = ind, aes(fill = newIndex), color = "black", linewidth = 0.2) +           # data tracts
#   geom_sf(data = background, fill = NA, color = "grey95", color = "grey80", linewidth = 0.1) +              # background tracts
#   geom_sf(data = lakes, fill = "grey95", color = NA) +                                    # water bodies
#   scale_fill_viridis_c(name = "Transportation Index", limits = c(0, 100)) + ### not sure if it's right to set this scale for the color pallette here.
#   labs(title = "Map of Census Tracts by Composite Transportation Index") +
#   theme_minimal() +
#   coord_sf(xlim = zoom_xlim, ylim = zoom_ylim, expand = FALSE)
 
```


The map shows a clear urban/rural divide in transportation accessibility. Census tracts in central urban areas, especially those near downtown Appleton and Oshkosh score much higher on the index, which suggests better walkability, transit access, and lower car dependency. On the other hand, rural tracts show consistently low index scores. This is what we would expect, but raises questions about whether the current index over penalizes low density areas where walking or public transit may not be practical options. The subjectivity of score thresholds (deciding what is “good” or “average”) in our new scoring process may influence final rankings and this might be something to look at further. The equal weighting of all components assumes each variable contributes equally to transportation reliability. This might be something to look into more closely as some factors for example car dependency might dominate a tract’s score.

\newpage

```{r}
# Drop geometry, rename columns, keep only the scores
scoreTbl2 <- ind %>% 
  st_drop_geometry() %>% 
  rename(
    Tract           = GEOID,
    WalkBike = actScore,
    DriveAlone   = carScore,
    UsePublic  = pubScore,
    Commute   = trvScore,
    NoVehicle = nvcScore,
    Walkability  = ekwScore,
    Overall    = newIndex
  ) %>% 
  select(Tract, WalkBike, DriveAlone, UsePublic,
         Commute, NoVehicle, Walkability, Overall)

# Slice out top-10 and bottom-10
top102    <- scoreTbl2 %>% arrange(desc(Overall)) %>% slice_head(n = 10)
bottom102 <- scoreTbl2 %>% arrange(Overall)        %>% slice_head(n = 10)

# Render with kable, putting most args on one line
knitr::kable(top102,    caption = "Top 10 Tracts",    digits = 1, booktabs = TRUE) %>% 
  kable_styling(full_width = FALSE)


knitr::kable(bottom102, caption = "Bottom 10 Tracts", digits = 1, booktabs = TRUE) %>% 
  kable_styling(full_width = FALSE)
```

# Potential Concerns & Further Exploration:

Some areas may score lower not because they lack access, but because certain types of infrastructure like public transit aren’t as relevant in those areas. For example, rural farmland would not need to have the same infrastructure as an urban area to to be considered accessible.