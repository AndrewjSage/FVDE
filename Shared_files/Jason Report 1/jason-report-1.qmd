---
title: "First Report (Preliminary Copy)"
author: "Mina and Binh"
format: pdf
---

```{r setup, include=F}
# Add packages you want here bru
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, fig.height = 10)
options(scipen = 7)
library(tidyverse)
library(stargazer)
library(corrplot)
library(patchwork)
library(tigris)
library(sf)
library(factoextra)
library(kableExtra)
```

# Introduction

This report is a short recap of the work we have done since our first meeting. As discussed, we have chosen to cover 2 categories for this report: humane housing and lifelong learning, at the zipcode level. For each category, we will perform some exploratory data analysis, explain our methodologies and create a preliminary index and accompanying visualizations.

# Humane Housing

## Legend:

Below is the full list of the variables available on the Data Exchange website related to the Humane Housing topic:

-   `APR` is the mortgage approval rate (given in percentages)
-   `EJV` is the vulnerable demographic index measured against other zipcodes in the country. A lower value indicates a lower percentage of vulnerable residents.
-   `EKW` is the walkability index, where higher values indicate higher walkability. `TRV` is the mean travel time to work.
-   `EVR` is the eviction rate, given in percentages.
-   `HBS` and `HBU` are percentages of residents under (severe) housing cost burden, or the proportion of residents spending a substantial amount of income on their mortgage. `RBU` and `RBS` are the rent-based version of this measure.
-   `HCP` is the proportion of renter-occupied housing units acquired through the federal housing choice voucher.
-   `HUO` is the percentage of housing units owned by their residents.
-   `MHC` is the median monthly housing cost, all inclusive (bills, rent, mortgage,...). `RNT` is the median rent.
-   `RFM` is the median home sale price, while `VAL` is the median home value in each respective zipcode.
-   `SLA-S` is the percentage of seniors living alone in their housing units.
-   `REX` is the race-ethnicity diversity index, or the probability of two residents in an area belonging to different cultural backgrounds.
-   `PGS` is the number of supermarkets within the zipcode.

```{r}
# Function to take the column names
colnames <- names(read.csv("hh zipcodes.csv", nrows = 0))

# Read the data, skipping the first two rows, then re-assign names
hh <- read.csv("hh zipcodes.csv", skip = 2, header = F)
names(hh) <- colnames
hh <- hh %>% select(-Layer, -Name)

# Creating other dataframes that fit the requirements
hhCorr <- hh %>% select(-PGS_2024, -HCP_2023) %>% na.omit() 
hhSumm <- hh %>% select(-GEOID) 
```

## Exploratory Plots and Tables

### Summary Statistics

Having given basic variable definitions, we now move on to summary statistics:


```{r summaryStats, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(knitr)
library(kableExtra)

# 1. Build the statTbl exactly as before
statTbl2 <- hhSumm %>%
  summarise(across(
    everything(),
    list(
      N      = ~sum(!is.na(.)),
      Mean   = ~mean(.,   na.rm = TRUE),
      StDev  = ~sd(.,     na.rm = TRUE),
      Min    = ~min(.,    na.rm = TRUE),
      Q1     = ~quantile(., 0.25, na.rm = TRUE),
      Median = ~median(., na.rm = TRUE),
      Q3     = ~quantile(., 0.75, na.rm = TRUE),
      Max    = ~max(.,    na.rm = TRUE),
      PctNA  = ~mean(is.na(.)) * 100
    ),
    .names = "{.col}_{.fn}"
  )) %>%
  pivot_longer(
    cols          = everything(),
    names_to      = c("Variable","Statistic"),
    names_pattern = "(.+)_(.+)"
  ) %>%
  pivot_wider(
    names_from  = Statistic,
    values_from = value
  ) %>%
  relocate(Variable, N, PctNA, Mean, StDev, Min, Q1, Median, Q3, Max) %>%
  mutate(
    N     = as.integer(N),
    PctNA = round(PctNA, 1),
    across(c(Mean, StDev, Min, Q1, Median, Q3, Max), ~ round(., 2))
  )

# 2. Print with kableExtra
statTbl2 %>%
  kable(
    format    = "latex",
    booktabs  = TRUE,
    caption   = "Summary Statistics with \\% Missing and IQR",
    align     = c("l", rep("r", 9))
  ) %>%
  kable_styling(
    latex_options   = c("scale_down", "hold_position"),
    font_size       = 7,
    full_width      = TRUE
  )
```

2 variables, the number of grocery stores (`PGS`) and housing voucher usage (`HCP`) are unavailable for most zipcodes.

### Histograms

Another way to visualize the data distribution is through histograms, which are provided below for the numeric variables sans `PGS` and `HCP`:

```{r, fig.width=12, fig.height=10}
hhvars <- c("APR_2023", "EJV_2024", "EKW_2024",     
  "EVR_2018", "HBS_2019.2023", "HBU_2019.2023",
  "HUO_2019.2023", "MHC_2019.2023", "RBS_2019.2023",
  "RBU_2019.2023", "REX_2019.2023", "RFM_2025.02",
  "RNT_2019.2023", "SLA.S_2019.2023","TRV_2019.2023",
  "VAL_2019.2023")

hhbins <- function(x) {
  diff(range(x, na.rm = TRUE)) / (2 * IQR(x, na.rm = TRUE) / length(x)^(1/3))
} #using the Freedman-Diaconis rule to compute the binwidth

hhplots <- map(hhvars, ~{
  varname <- .x
  binwidth <- round(hhbins(hhSumm[[varname]]))
  
  ggplot(data=hhSumm, aes_string(x=varname)) + 
    geom_histogram(fill="#2196f3", color="#000000", bins=binwidth) + 
    labs(title=varname, x=NULL, y="Count")
})

wrap_plots(hhplots, ncol = 4, nrow = 4) & plot_annotation(caption="*Note: histogram binwidth was decided using the Freedman-Diaconis rule.")
```

Observations:

-   Right-tailed distribution of eviction rates (`EVR`): some neighborhoods see much higher eviction rates than others in the dataset.

-   Mortgage approval (`APR`) and monthly rent (`RNT`) are relatively normally distributed

-   Variables related to housing costs and house values are predictably right-skewed: one would expect some zipcodes to have much higher house prices or rent rates compared to others (good location, downtowns,...) 

-   `HBU` shows a right-tailed distribution, while `HBS` is much less so.   

### Correlation Plot:

```{r, fig.width=10, fig.height=10}

A = cor(hhCorr)
corrplot(A, method='color', order = 'AOE', type='upper', addCoef.col = 'black')
```


Observations:

-   Variables related to housing costs show covariance: median house values (`VAL`) and median house prices (`RFM`) are understandably linked, and the link between these variables and rent (`RNT`) or mortgage costs are also easily explained.

-   `HBU`, `HBS`, `RBU`, and `RBS` - variables related to the proportion of the population under housing cost burden with different cutoffs - also show high levels of covariance. The link between them are also understandable: the `HBS` and `RBS` use the same methodology as `HBU` and `RBU`, just with different numeric thresholds.

-   Home owner occupation (`HUO`) has a strong negative correlation with demographic vulnerability (`EJV`) and neighborhood walkability (`EKW`). This is because areas with high levels of home ownership would on average be in better economic conditions, and hence less reliant on walking or public transportation.


## Normalization and Z-score calculations

To ensure the informativeness of our proposed indices, we would need to perform some transformations of the variables. This is to ensure that higher values of any variable would indicate a better outcome. Below is the entire list of variables that are negated or transformed and some motivation behind why:

-   `EJV_2024` is the vulnerable demographic index - a higher value implies a greater percentage of vulnerable residents, and hence, should be negated.

-   `EVR_2018`: high eviction rates generally imply less stability and a less desirable outcome for human housing.

-   `HBS`, `HBU`, `RBS`, `RBU` are variables measuring the proportion of the population under duress from housing costs or rent. Higher values are less desirable, and hence should be negated. 

-   `HCP_2023`: usage of housing choice vouchers indicates need or general socioeconomic pressures. However, higher estimates might also imply higher access to safe and fair housing. Current decision is to negate this variable (implies extreme poverty)

-   `MHC`, `RFM`, `RNT` and `VAL` are variables related to the costs of purchasing or renting housing, and so even though some arguments can be made about these variables reflecting the standard of living across zipcodes, the current decision is to negate these variables as well. This decision can be reverted with confirmation 

-   `SLA-S` is the proportion of elders living alone. From the lens of social-support structures (or lack thereof), higher values would indicate more isolated seniors and of a less desirable outcome.

-   `TRV_2019-2023` is the average commute time. Higher values indicate longer commutes, which have been shown to negatively affect quality of life.

```{r}
# Variables to flip:
flipVars <- c("EJV_2024", "EVR_2018", "HBS_2019.2023", "HBU_2019.2023",
  "MHC_2019.2023", "RBS_2019.2023", "RBU_2019.2023", "RFM_2025.02",
  "RNT_2019.2023", "SLA.S_2019.2023", "TRV_2019.2023", "VAL_2019.2023")

# across(all_of()) is my new favorite thing ever
#.x points to the current column in the new column name vector
hhNorm <- hh %>% mutate(across(all_of(flipVars), ~ - .x))

# z score calculations, miraculous formula
hhNorm <- hhNorm %>%
  mutate(across(all_of(names(.)),
                ~ ( .x - mean(.x, na.rm = TRUE) ) / sd(.x, na.rm = TRUE), 
                .names = "z_{.col}")) %>% select(-z_GEOID)
```

\newpage
## Aggregation and Visualization

### Aggregation
```{r}
zCols <- names(hhNorm)[startsWith(names(hhNorm), "z_")]

# 2. Impute NAs in those columns with 0, then compute the index
hhNorm <- hhNorm %>%
  mutate(across(all_of(zCols), ~ coalesce(.x, 0))) %>%
  mutate(housingIndex = rowMeans(across(all_of(zCols))))

# 3. Extract final GEOID + index
hhFinal <- hhNorm %>%
  select(GEOID, housingIndex)

cols <- c(
  "GEOID",
  "housingIndex",
  "z_VAL_2019.2023",
  "z_RFM_2025.02",
  "z_MHC_2019.2023",
  "z_HBS_2019.2023")

```
The current approach is to average all of the available z-scores.

### Visualization

Below is a map containing the humane housing index for all of the zipcodes:

```{r, fig.width=10, fig.height=8}

# Getting spatial data from tigris, zcta was found from reading the documentation
zips <- unique(hhFinal$GEOID)
spatial <- zctas(state="wi", class="sf", year=2010, progress_bar=F)
foxZips <- spatial %>% mutate(ZCTA5CE10=as.character(ZCTA5CE10)) %>% filter(ZCTA5CE10 %in% zips)
# Getting other zips from tigris so that we know where we are
background <- spatial %>%
  mutate(ZCTA5CE10=as.character(ZCTA5CE10)) %>%
  filter(!ZCTA5CE10 %in% zips)

# joining the data tables together
hhFinal <- hhFinal %>% mutate(GEOID=as.character(GEOID)) %>%  left_join(foxZips, by=c("GEOID"="ZCTA5CE10"))

hhFinal <- hhFinal %>% st_as_sf()

# Arbitrary limits decided by AI
zoom_xlim <- c(-89.5, -87.5)
zoom_ylim <- c(43.5, 45.0)

# Drawing up the map
ggplot() +
  geom_sf(data = hhFinal, aes(fill = housingIndex), color="black") +
  geom_sf(data = background, fill = "grey95", color = "grey80", linewidth = 0.1) +
  scale_fill_viridis_c() + 
  labs(title = "Map of Zipcodes by Composite Housing Index") +
  theme_minimal() + coord_sf(xlim = zoom_xlim, ylim = zoom_ylim, expand = FALSE)

```

Here are 2 tables containing the top and bottom 10 zipcodes according to our preliminary index:
```{r, results='asis'}

top10 <- hhNorm %>%
  arrange(desc(housingIndex)) %>%
  slice_head(n = 10) %>%
  select(all_of(cols))

top10 %>%
  kable(
    format   = "latex",
    booktabs = TRUE,
    caption  = "Top 10 ZIP Codes by Housing Index",
    digits   = 2
  ) %>%
  kable_styling(
    font_size     = 10
  )

bottom10 <- hhNorm %>%
  arrange(housingIndex) %>%
  slice_head(n = 10) %>%
  select(all_of(cols))

bottom10 %>%
  kable(
    format   = "latex",
    booktabs = TRUE,
    caption  = "Bottom 10 ZIP Codes by Housing Index",
    digits   = 2
  ) %>%
  kable_styling(
    font_size     = 10
  )
```


# Lifelong Learning

Now we turn our attention to the lifelong learning dataset.

```{r}
# getting the data into readable format
colnames <- names(read.csv("Lifelong-Learning-Zip-Codes.csv",nrows=0))

# data wrangling blah blah blah
balls <- read.csv("Lifelong-Learning-Zip-Codes.csv", skip=2, header=FALSE)
names(balls) <- colnames
balls <- balls |> select(-Layer, -Name) # removing redundant columns

# Wrangled ballsframes for other visualizations
ballsCorr <- balls |> select(-FVDEWVAR_2023.2024, -FVDEYLCV_2024.2025, -GEOID, -Longitude, -Latitude, -Population) |> na.omit()
ballsCorr1 <- balls |> select(-GEOID, -Longitude, -Latitude, -Population) |> na.omit()

ballsSumm <- balls |> select(-GEOID, -Longitude, -Latitude)
```

## Legend

The variables covered in the dataset are presented below. I'll give the letters below while the corresponding numbers following them are the relevant years.

-   `GEOID`: Corresponds with the zip code
-   `EDA`: 9th grade education rate (%)
-   `EDB`: high school graduation rate (%)
-   `EDC`: any higher education rate (%)
-   `EDE`: college graduation rate (%)
-   `EDG`: preschool enrollment for ages 3-4 (%)
-   `FWDEWVAR`: public school suspensions K-12
-   `FVDEYLCV`: public school enrollment K-12

It is worth noting that there is a significant number of `NA` entries in the last two variables.

```{r}
# Create a df with the number of na entries for each variable
na_summary <- colSums(is.na(balls)) |> as.data.frame() |> rename('na_count' = 'colSums(is.na(balls))')

na_summary <- na_summary |> mutate("na_percent" = (na_count / 50) * 100)
kable(na_summary[order(na_summary$na_percent, decreasing=TRUE),], caption="Number and percent of NA entries")
```

## Exploratory Plots and Variables

Here are a few pictures to help us get to know the data a bit.

### Correlation

We can look at the correlation between variables. Due to the large number of `NA` entries in some of the variables (i.e. `FWDEWVAR`,`FVDEYLCV`), it may not be worth looking at the data both with and without them. Here is the correlation plot with the final two variables omitted.

```{r, fig.height=5}
# Without the last two variables
plt = cor(ballsCorr)
corrplot(plt, method='color', order = 'AOE', type='upper', addCoef.col = 'black')
```

Unsurprisingly, there is a high correlation between the any higher education rate `EDC` and the college graduation rate `EDE` (0.84). There also appears to be a high correlation between the high school graduation rate `EDB` and any higher education rate `EDC` (0.71).

Here is the correlation plot including the last two variables. Due to the large number of `NA` entries, the number of rows has been shaved down from 50 to 25.

```{r, fig.height=5}
# With the last two variables
plt1 = cor(ballsCorr1)
corrplot(plt1, method='color', order = 'AOE', type='upper', addCoef.col = 'black')
```

Again, we see a high correlation between the any higher education rate `EDC` and college graduation rate `EDE` (0.91). There is also high correlation between the 9th grade graduation rate `EDA` and high school graduation rate `EDB` (0.77), high school graduation rate `EDB` and any higher education rate `EDC` (0.76), and public school suspensions `FVDEWVAR` and public school enrollment `FVDEYLCV` (0.78).

### Histograms

We can also look at a histogram for each variable.

```{r, fig.width=9, fig.height=4}
vars <- c("EDA_2019.2023", "EDB_2019.2023", "EDC_2019.2023" ,"EDE_2019.2023", "EDG.X341_2019.2023", "FVDEWVAR_2023.2024", "FVDEYLCV_2024.2025")

bins_fd <- function(x) {
  diff(range(x, na.rm = TRUE)) / (2 * IQR(x, na.rm = TRUE) / length(x)^(1/3))
} #using the Freedman-Diaconis rule to compute the binwidth

plots <- map(vars, ~{
  varname <- .x
  binwidth <- round(bins_fd(balls[[varname]]))
  
  ggplot(data=balls, aes_string(x=varname)) + 
    geom_histogram(fill="#2196f3", color="#000000", bins=binwidth) + 
    labs(title=varname, x=NULL, y="Count")
})

wrap_plots(plots, ncol = 4, nrow = 2) & plot_annotation(caption="*Note: histogram binwidth was decided using the Freedman-Diaconis rule.")
```

We can see there is skewedness in some of the variables, like the 9th grade graduation rate `EDA`, preschool enrollment rate `EDG`, public school suspensions `FVDEWVAR` and public school enrollment `FVDEYLCV`. For the variables public school suspensions `FVDEWVAR` and public school enrollment `FVDEYLCV`, the skewedness may be explained by the fact that the data is in counts, not percentages. Thus, the value could be reflective of the population of a zip code. It is possible that many zip codes are of similar population except for a handful that have more people. An investigation into the number of residents that would be attending public school at each zip code would be needed to confirm this speculation.

There also appears to be outliers in the high school graduation rate `EDB` and preschool enrollment `EDG`. Upon closer inspection at the data, the lower high school graduation rate `EDB` corresponds to zip codes 54985 and 54152, while the high preschool enrollment `EDG` also occurs in zip code 54152.

### Summary statistics

```{r, results='asis'}
stargazer(ballsSumm, title="Summary Stats", type = 'latex', summary.stat = c("n","mean","sd", "min", "median", "max", "p25", "p75"), font.size = "small", notes.append=F, header=F)
```

## Normalization and Z-Score Calculations

Because we want larger numbers to correspond to better living, some variables may need to be flipped.

The only variable I will flip is the `FVDEWVAR` which corresponds to the number of public school suspensions in K-12, as many would consider a higher number of suspensions to be a bad thing.

```{r}
#flipping what needs to be flipped
flipVars <- c("FVDEWVAR_2023.2024")

ballsNorm <- balls |> mutate(across(all_of(flipVars), ~ - .x))

#z-score calculations wowge
ballsNorm <- ballsNorm |> 
  mutate(across(all_of(names(balls)),
                ~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE), 
                .names = "z_{.col}")) |> select(-z_GEOID, -z_Longitude, -z_Latitude, -z_Population)

zscore_names <- names(ballsNorm)[startsWith(names(ballsNorm), "z_")] # names of zscore cols
zscores <- ballsNorm |> select(GEOID, zscore_names)
```

## Indexing and Visualization

We move on to the indexing process for our dataset. Here, we simply compute the z-scores for each of the variables and take an average in the same manner as we did for the humane housing condition. There are seven variables (as described previously), and therefore seven z-scores. Perhaps there is more to experiment on with weights, but we leave that to another day.

The following tables display the z-scores of the concerned variables as well as the index by zip code. They include the 10 zip codes with the highest index and the 10 zip codes with the lowest index. Please note that when a zip code has a missing value, that variable is not included in the average (for that zip code).

```{r}
zscoreIndex <- data.frame('GEOID'=zscores[,1], Index=rowMeans(zscores[,-1], na.rm=TRUE))
index <- zscores |> left_join(zscoreIndex, by = "GEOID") |> arrange(desc(Index))
names(index)[2:8] <- c("EDA", "EDB", "EDC", "EDE", "EDG", "FVDEWVAR", "FVDEYLCV")

# Top 10 zscores
kable(head(index, 10), caption = "Top 10 indexes (by zipcode)", digits=3)
```

```{r}
# Bottom 10 zscores
kable(tail(index, 10), caption = "Bottom 10 indexes (by zipcode)", digits=3)
```

It is worth noting that the variables public school suspensions `FVDEWVAR` and public school enrollment `FVDEYLCV` are in counts, so larger areas with more people and bigger schools will likely have more enrollment and consequently more suspensions. This is something to consider for the index; because the value is not a percentage, larger areas may get penalized/rewarded disproportionately for their student count/suspension count.

We can display the indexes on a map for an intuitive visualization.

```{r, fig.width=10, fig.height=10, fig.cap="Map of the lifelong learning index (z-score) by zip code"}
# Making the map for the index computed by averaging the z-scores

zscoreIndex <- data.frame('GEOID'=zscores[,1], Index=rowMeans(zscores[,-1], na.rm=TRUE))

# Getting spatial data from tigris, zcta was found from reading the documentation
zips <- unique(zscoreIndex$GEOID)
spatial <- zctas(state="wi", class="sf", year=2010, progress_bar=F)
foxZips <- spatial %>% mutate(ZCTA5CE10=as.character(ZCTA5CE10)) %>% filter(ZCTA5CE10 %in% zips)
# Getting other zips from tigris so that we know where we are
background <- spatial %>%
  mutate(ZCTA5CE10=as.character(ZCTA5CE10)) %>%
  filter(!ZCTA5CE10 %in% zips)

# joining the data tables together
zscoreIndex <- zscoreIndex %>% mutate(GEOID=as.character(GEOID)) %>%  left_join(foxZips, by=c("GEOID"="ZCTA5CE10"))

zscoreIndex <- zscoreIndex %>% st_as_sf()

# Arbitrary limits decided by ME!!! (so, not arbitrary)
zoom_xlim <- c(-89.2, -87.8)
zoom_ylim <- c(43.7, 44.9)

# Drawing up the map
zs_map <- ggplot() +
  geom_sf(data = zscoreIndex, aes(fill = Index), color="black") +
  geom_sf(data = background, fill = "grey95", color = "grey80", linewidth = 0.1) +
  scale_fill_viridis_c() + 
  labs(title = "Lifelong Learning Index") +
  theme_minimal() + coord_sf(xlim = zoom_xlim, ylim = zoom_ylim, expand = FALSE)

zs_map
```

We can see pretty clearly that zip codes 54169 and 54942 have the highest lifelong learning index, as shown in bright yellow. In fact, they are the only zip codes that have an index higher than 1. On the other hand, while barely visible, the tiny area Winnebago (zip code 54985) has the lowest lifelong learning index of -2.668. Most of the indexes do not go below -1, with a great majority clustering between 0.5 and -0.5. Overall, the lifelong learning index does not seem to vary too much between these zip codes.
