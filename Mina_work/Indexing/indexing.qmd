---
title: "All the Indexing on all the different levels"
author: cheese
format: pdf
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = F, message=FALSE, warning=FALSE)
options(scipen = 5)
library(tidyverse)
library(stargazer)
library(corrplot)
library(patchwork)
library(tigris)
library(sf)
library(factoextra)
library(kableExtra)
```

Gonna do the indexing scheme from Jason Report 2 for all the subdivisions here. We gonna skip straight to the indexing, ignoring preliminary analysis as seen in the previous reports, as well as the maps. The data is all collected from 2019-2023.

# Legend
 
The variable names do not provide much insight into what they are representing. Thus, I will describe them here. Note that not all the variables appear in all the datasets.

-   `EDA`: 9th grade education rate (% of residents)
-   `EDB`: High school graduation rate (% of residents)
-   `EDC`: Any higher education rate (% of residents)
-   `EDE`: College graduation rate (% of residents)
-   `EDG`: Preschool enrollment rate (% of toddlers ages 3-4)
-   `CHF`: School funding adequacy (dollars) 
-   `CHS`: School segregation index
-   `WVAR`: Public school suspensions (count)
-   `YLCV`: Public school enrollment (count)

Because `WVAR` public school suspensions and `YLCV` public school enrollment are in counts, I decided to omit them since areas with larger populations will be overly represented/punished, meaning they would add extra convolution to the index. If we can get those numbers in percentages, that would be a different story. Additionally, for the sake of time and simplicity, I will stick to just the `EDX` (first five) variables for this, as they are present in every dataset.

# Scheme

We use an indexing scheme in order to get our indexes between 0-100. The baselines are displayed below.

```{r}
condition <- read.csv("Condition-baseline-llct.csv") # add variables from the other datasets?

kable(condition)
```

Note that the values for the conditions are decided arbitrarily and the values for each of the variables are guesstimated to the best of my ability by cross referencing multiple online sources. The average corresponds to the average as reported by the American Community Survey (ACS) in 2023, and the other quintiles are judged based on other stuff I found online. IF someone knows where to find all that location data please just send me the file ok so i can do that Q1 Q3 stuff for a more "statistically accurate" baseline or whatever the fuck.

We can then use these baseline values to create a function that maps the values we have for each variable to their condition score. For simplicity's sake, I'm gonna go with a polygonal approximation (sorry Lagrange). The graphs below illustrate the idea.

```{r}
#file orwhatever for census tracts
# getting the data into readable format
colnames <- names(read.csv("Lifelong-Learning-Cencus-Tracts.csv",nrows=0))

# data wrangling blah blah blah
balls <- read.csv("Lifelong-Learning-Cencus-Tracts.csv", skip=2, header=FALSE)
names(balls) <- colnames
balls <- balls |> select(-Layer) # removing redundant columns
names(balls)[4:8] <- c("EDA", "EDB", "EDC", "EDE", "EDG") # get rid of the stupid date things since they're all the same (see below)

# data for the prelim analysis
ballsAnal <- balls |> select(-GEOID, -Name)
```

```{r, fig.height=3, fig.width=15}
vars_cond <- c("EDA", "EDB", "EDC", "EDE", "EDG")

cond_plots <- map(vars_cond, ~{
  cdvar <- .x
  
  ggplot(data=condition, aes_string(x=cdvar, y='Cond_Value')) +
    geom_line() +
    geom_point() +
    labs(title=paste(cdvar, "Score"), x=paste(cdvar, "Value"), y="Condition Score") + scale_x_continuous(limits = c(0,NA))
})

wrap_plots(cond_plots, ncol=5, nrow=1)
```

```{r}
# Here I write a function to do the indexing for me so I don't have to hard code the data frames every single time

# baseline_df: data frame with baseline explanatory variables and Cond_Value
# target_df: data frame where you want interpolated values
# vars_cond: character vector of variable names
# scores_only: Only includes interpolated scores and not the original values
# remove_suffix: removes the ending _CondScore
# add_index: includes the index in final table
# ind_name: name index column

interpolate_scores <- function(
    baseline_df, target_df, vars_cond,
    scores_only = TRUE, remove_suffix = TRUE,
    add_index = TRUE, ind_name = "Index"
) {
  
  for (var in vars_cond) {
    
    # Baseline values
    baseline_x <- baseline_df[[var]]
    baseline_y <- baseline_df$Cond_Value
    valid_rows <- !is.na(baseline_x) & !is.na(baseline_y)
    
    # Interpolation function
    interp_fun <- approxfun(
      x = baseline_x[valid_rows], 
      y = baseline_y[valid_rows], 
      rule = 2
    )
    
    # New column name
    new_colname <- paste0(var, "_CondScore")
    
    # Apply interpolation
    target_df[[new_colname]] <- interp_fun(target_df[[var]])
  }
  
  # Keep only scores if requested, but preserve name & GEOID
  if (scores_only) {
    keep_cols <- c("Name", "GEOID", grep("_CondScore$", names(target_df), value = TRUE))
    target_df <- target_df %>%
      select(all_of(keep_cols))
    
    # Remove suffix if requested
    if (remove_suffix) {
      score_cols <- setdiff(names(target_df), c("Name", "GEOID"))
      target_df <- target_df %>%
        rename_with(~ sub("_CondScore$", "", .x), all_of(score_cols))
    }
  }
  
  # Add average score if requested
  if (add_index) {
    score_cols <- setdiff(names(target_df), c("Name", "GEOID"))
    target_df <- target_df %>%
      rowwise() %>%
      mutate(!!ind_name := mean(c_across(all_of(score_cols)), na.rm = TRUE)) %>%
      ungroup()
  }
  
  return(target_df)
}

```

# Indexing - Census Tracts

We start with the census tract layer

```{r}
#test
balls_index <- interpolate_scores(condition, balls, vars_cond)|> arrange(desc(Index))
```

After some coding that you cannot see, we can display the top 10 and bottom 10 indexes computed using this methodology. The following tables show these condition scores of each of the variables as well as the index, which is an average of those scores.

```{r}
# Top 10 indexes
kable(head(balls_index, 10), caption = "Top 10 indexes (by Census Tract)", digits=3)
```

```{r}
# Bottom 10 indexes
kable(tail(balls_index, 10), caption = "Bottom 10 indexes (by Census Tract)", digits=3)
```

# Indexing - Cities and Towns

```{r}
#file orwhatever
# getting the data into readable format
colnames <- names(read.csv("Lifelong-Learning-Cities-and-Towns.csv",nrows=0))

# data wrangling blah blah blah
CnT <- read.csv("Lifelong-Learning-Cities-and-Towns.csv", skip=2, header=FALSE)
names(CnT) <- colnames
CnT <- CnT |> select(-Layer) # removing redundant columns
names(CnT)[4:10] <- c("EDA", "EDB", "EDC", "EDE", "EDG", "WVAR", "YLCV") # get rid of the stupid date things since they're all the same (see below)

# data for the prelim analysis
CnTAnal <- CnT |> select(-GEOID, -Name)
```



# Indexing - Zip Codes

```{r}
#file orwhatever
# getting the data into readable format
colnames <- names(read.csv("Lifelong-Learning-Zip-Codes.csv",nrows=0))

# data wrangling blah blah blah
zc <- read.csv("Lifelong-Learning-Zip-Codes.csv", skip=2, header=FALSE)
names(zc) <- colnames
zc <- zc |> select(-Layer) # removing redundant columns
names(zc)[6:12] <- c("EDA", "EDB", "EDC", "EDE", "EDG", "WVAR", "YLCV") # get rid of the stupid date things since they're all the same (see below)

# data for the prelim analysis
zcAnal <- zc |> select(-GEOID, -Name)
```



# Indexing - Counties

```{r}
#file orwhatever
# getting the data into readable format
colnames <- names(read.csv("Lifelong-Learning-Counties.csv",nrows=0))

# data wrangling blah blah blah
cntz <- read.csv("Lifelong-Learning-Counties.csv", skip=2, header=FALSE)
names(cntz) <- colnames
cntz <- cntz |> select(-Layer) # removing redundant columns
names(cntz)[4:12] <- c("CHF", "CHS", "EDA", "EDB", "EDC", "EDE", "EDG", "WVAR", "YLCV") # get rid of the stupid date things since they're all the same (see below)

# data for the prelim analysis
cntzAnal <- cntz |> select(-GEOID, -Name)
```



# Indexing - Regions

CHF - School funding adequacy (dollars)
CHS - School segregation index

```{r}
#file orwhatever
# getting the data into readable format
colnames <- names(read.csv("Lifelong-Learning-Regions.csv",nrows=0))

# data wrangling blah blah blah
rgns <- read.csv("Lifelong-Learning-Regions.csv", skip=2, header=FALSE)
names(rgns) <- colnames
rgns <- rgns |> select(-Layer) # removing redundant columns
names(rgns)[4:12] <- c("CHF", "CHS", "EDA", "EDB", "EDC", "EDE", "EDG", "WVAR", "YLCV") # get rid of the stupid date things since they're all the same (see below)

# data for the prelim analysis
rgnsAnal <- rgns |> select(-GEOID, -Name)
```


## qUESTIONS: SHOULD I KILL MYSELF????
ANSWER: ?

- INCORPORATE OTHER VARIABLES THAT ARE ONLY IN SOME DATASETS INTO INDEX?
- SOME OF THE VARS ARE SUS LIKE WTF IS SEGRATATION INDEX HELLO??????????????????d
- idk where to get baselins for allat bigbro









