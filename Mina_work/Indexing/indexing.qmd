---
title: "all the stuff idk"
author: cheese
format: pdf
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = F, message=FALSE, warning=FALSE)
options(scipen = 5)
library(tidyverse)
library(stargazer)
library(corrplot)
library(patchwork)
library(tigris)
library(sf)
library(factoextra)
library(kableExtra)
```

Gonna do the indexing scheme from Jason Report 2 for all the subdivisions here. We gonna skip straight to the indexing 

```{r}
#file orwhatever
# getting the data into readable format
colnames <- names(read.csv("Lifelong-Learning-Cencus-Tracts.csv",nrows=0))

# data wrangling blah blah blah
balls <- read.csv("Lifelong-Learning-Cencus-Tracts.csv", skip=2, header=FALSE)
names(balls) <- colnames
balls <- balls |> select(-Layer) # removing redundant columns
names(balls)[4:8] <- c("EDA", "EDB", "EDC", "EDE", "EDG") # get rid of the stupid date things since they're all the same (see below)

# data for the prelim analysis
ballsAnal <- balls |> select(-GEOID, -Name)
```

## Legend
 
THe variable names do not provide much insight into what they are representing. Thus, I will describe them here. Please note that all of this data was collected from 2019-2023.

-   `EDA`: 9th grade education rate (% of residents)
-   `EDB`: High school graduation rate (% of residents)
-   `EDC`: Any higher education rate (% of residents)
-   `EDE`: College graduation rate (% of residents)
-   `EDG`: Preschool enrollment rate (% of toddlers ages 3-4)

## Indexing - Census Tracts

We can try to do an indexing using an arbitrarily decided scale, so we can get indexes from 1-100...

```{r}
condition <- read.csv("Condition-baseline-llct.csv")

kable(condition)
```

Note that the values for the conditions are decided arbitrarily and the values for each of the variables are guesstimated to the best of my ability by cross referencing multiple online sources. The any higher education rate `EDC` variable is especially guesstimated, as almost all of the information I found online was about high school degree+ or bachelor's degree+.

We can then use these baseline values to create a function that maps the values we have for each variable to their condition score. For simplicity's sake, I'm gonna go with a polygonal approximation (sorry Lagrange). The graphs below illustrate the idea.

```{r, fig.height=3, fig.width=15}
vars_cond <- c("EDA", "EDB", "EDC", "EDE", "EDG")

cond_plots <- map(vars_cond, ~{
  cdvar <- .x
  
  ggplot(data=condition, aes_string(x=cdvar, y='Cond_Value')) +
    geom_line() +
    geom_point() +
    labs(title=paste(cdvar, "Score"), x=paste(cdvar, "Value"), y="Condition Score") + scale_x_continuous(limits = c(0,NA))
})

wrap_plots(cond_plots, ncol=5, nrow=1)
```

```{r}
# Code from ChatGPT.. im sorry i cant code... TT

# For each variable, build interpolation function and apply to balls
for (var in vars_cond) {
  
  # Filter condition data for just this variable (non-NA values only)
  baseline_x <- condition[[var]]
  baseline_y <- condition$Cond_Value
  valid_rows <- !is.na(baseline_x) & !is.na(baseline_y)
  
  # Build interpolation function.. approxfun does piecewise linear interpolating
  interp_fun <- approxfun(x = baseline_x[valid_rows], 
                          y = baseline_y[valid_rows], 
                          rule = 2)  # rule = 2 allows extrapolation using nearest value; we need this because some of the values fall below the terrible part, meaning they have a score of 0 (won't show up as na)
  
  # Apply interpolation to balls
  new_colname <- paste0(var, "_CondScore")
  balls[[new_colname]] <- interp_fun(balls[[var]])
}
```

```{r}
# dataset of just the condition scores + index (averaging everything)
indexes <- balls |> select("Name", "GEOID", ends_with("_CondScore")) |> 
  rowwise() |> mutate(Index = mean(c_across(ends_with("_CondScore")), na.rm = TRUE)) |>
  ungroup() |> arrange(desc(Index))

indexes <- indexes |> rename_with(~ gsub("_CondScore.*", "", .x))
```

After some coding that you cannot see, we can display the top 10 and bottom 10 indexes computed using this methodology. The following tables show these condition scores of each of the variables as well as the index, which is an average of those scores.

```{r}
# Top 10 zscores
kable(head(indexes, 10), caption = "Top 10 indexes (by Census Tract)", digits=3)
```

```{r}
# Bottom 10 zscores
kable(tail(indexes, 10), caption = "Bottom 10 indexes (by Census Tract)", digits=3)
```

## Visualization

We can visualize these indexes on a map so we have an intuitive way to compare them.

```{r, fig.width=10, fig.height=10, fig.cap="Map of the lifelong learning index (z-score) by census tract"}
# Making the map for the index computed by averaging the z-scores

condScores <- indexes |> select(GEOID, Index)

# Getting spatial data from tigris, zcta was found from reading the documentation
zips <- unique(condScores$GEOID)
spatial <- tracts(state="wi", class="sf", year=2023, progress_bar=F)
foxZips <- spatial %>% mutate(GEOID=as.character(GEOID)) %>% filter(GEOID %in% zips)
# Getting other zips from tigris so that we know where we are
background <- spatial %>%
  mutate(GEOID=as.character(GEOID)) %>%
  filter(!GEOID %in% zips)

# joining the data tables together
condScores <- condScores %>% mutate(GEOID=as.character(GEOID)) %>%  left_join(foxZips, by=c("GEOID"="GEOID"))

condScores <- condScores %>% st_as_sf()

# Arbitrary limits decided by ME!!! (so, not arbitrary)
zoom_xlim <- c(-89, -88)
zoom_ylim <- c(43.8, 44.7)

# Drawing up the map
cond_map <- ggplot() +
  geom_sf(data = condScores, aes(fill = Index), color="black") +
  geom_sf(data = background, fill = "grey95", color = "grey80", linewidth = 0.1) +
  scale_fill_viridis_c() + 
  labs(title = "Lifelong Learning Index (by Census Tract)") +
  theme_minimal() + coord_sf(xlim = zoom_xlim, ylim = zoom_ylim, expand = FALSE)

cond_map
```

## Indexing - Cities and Towns





## Indexing - Zip Codes





## Indexing - Counties





## Indexing - Regions













